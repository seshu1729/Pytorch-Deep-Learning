{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be57a0e9",
   "metadata": {},
   "source": [
    "- [Main code](#main-code)\n",
    "- [Step by step code flow](step-by-step-code-flow)\n",
    "- [Another way using transformers](another-way-using-transformers)\n",
    "- [Optional](#optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dea13f-e8c0-4adc-a56c-8d7aa192e636",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7f84fd5-921b-468d-b825-5e569038c2c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6925, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2246, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1637, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1363, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1201, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1092, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1011, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0949, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0899, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0857, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Evaluating on the training data\n",
      "accuracy: tensor(0.9789)\n",
      "sensitivity: tensor(0.9211)\n",
      "specificity: tensor(0.9881)\n",
      "precision: tensor(0.9241)\n",
      "Evaluating on the validation data\n",
      "accuracy: tensor(0.9749)\n",
      "sensitivity: tensor(0.9137)\n",
      "specificity: tensor(0.9836)\n",
      "precision: tensor(0.8881)\n",
      "tensor([[0.0518],\n",
      "        [0.6129],\n",
      "        [0.0134]])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "df_train = df.sample(frac=0.8, random_state=0)\n",
    "df_val = df.drop(index=df_train.index)\n",
    "\n",
    "cv = CountVectorizer(max_features=1000)\n",
    "messages_train = cv.fit_transform(df_train[\"message\"])\n",
    "messages_val = cv.transform(df_val[\"message\"])\n",
    "\n",
    "X_train = torch.tensor(messages_train.todense(), dtype=torch.float32)\n",
    "y_train = torch.tensor(df_train[\"spam\"].values, dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "X_val = torch.tensor(messages_val.todense(), dtype=torch.float32)\n",
    "y_val = torch.tensor(df_val[\"spam\"].values, dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "model = nn.Linear(1000, 1)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        print(loss)\n",
    "\n",
    "def evaluate_model(X, y): \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = nn.functional.sigmoid(model(X)) > 0.25\n",
    "        print(\"accuracy:\", (y_pred == y)\\\n",
    "            .type(torch.float32).mean())\n",
    "        \n",
    "        print(\"sensitivity:\", (y_pred[y == 1] == y[y == 1])\\\n",
    "            .type(torch.float32).mean())\n",
    "        \n",
    "        print(\"specificity:\", (y_pred[y == 0] == y[y == 0])\\\n",
    "            .type(torch.float32).mean())\n",
    "\n",
    "        print(\"precision:\", (y_pred[y_pred == 1] == y[y_pred == 1])\\\n",
    "            .type(torch.float32).mean()) \n",
    "        \n",
    "print(\"Evaluating on the training data\")\n",
    "evaluate_model(X_train, y_train)\n",
    "\n",
    "print(\"Evaluating on the validation data\")\n",
    "evaluate_model(X_val, y_val)\n",
    "\n",
    "custom_messages = cv.transform([\n",
    "    \"We have release a new product, do you want to buy it?\", \n",
    "    \"Winner! Great deal, call us to get this product for free\",\n",
    "    \"Tomorrow is my birthday, do you come to the party?\"\n",
    "])\n",
    "\n",
    "X_custom = torch.tensor(custom_messages.todense(), dtype=torch.float32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = nn.functional.sigmoid(model(X_custom))\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ffdb1f-aa0b-49cb-ba91-b09c0a42bd0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step by step code flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7957c24",
   "metadata": {},
   "source": [
    "First lets explore what is the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b93cee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 12, saw 4\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data/SMSSpamCollection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m df.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\env\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 2 fields in line 12, saw 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8194dd3e",
   "metadata": {},
   "source": [
    "As it was not csv by default, we need to handle the data correctly.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "951aea72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ham  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4  spam   \n",
       "\n",
       "  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
       "0                      Ok lar... Joking wif u oni...                                                               \n",
       "1  Free entry in 2 a wkly comp to win FA Cup fina...                                                               \n",
       "2  U dun say so early hor... U c already then say...                                                               \n",
       "3  Nah I don't think he goes to usf, he lives aro...                                                               \n",
       "4  FreeMsg Hey there darling it's been 3 week's n...                                                               "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/SMSSpamCollection\", sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd84106",
   "metadata": {},
   "source": [
    "It was taking the 1st entry into the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c36ed6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/SMSSpamCollection\", sep=\"\\t\", names=[\"type\", \"message\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52a483d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type                                                     ham\n",
       "message    Go until jurong point, crazy.. Available only ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87bd766b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3072b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2][\"message\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c4a14",
   "metadata": {},
   "source": [
    "How to know which message is spam. We can know it by checking type.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfbe3bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2        True\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "5567     True\n",
       "5568    False\n",
       "5569    False\n",
       "5570    False\n",
       "5571    False\n",
       "Name: type, Length: 5572, dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"type\"] == \"spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845d4157",
   "metadata": {},
   "source": [
    "To make the neuron learn easily, we can keep 0,1 in the column instead of ham and spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25852505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                            message   spam\n",
       "0      ham  Go until jurong point, crazy.. Available only ...  False\n",
       "1      ham                      Ok lar... Joking wif u oni...  False\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   True\n",
       "3      ham  U dun say so early hor... U c already then say...  False\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...  False\n",
       "...    ...                                                ...    ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...   True\n",
       "5568   ham               Will ü b going to esplanade fr home?  False\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...  False\n",
       "5570   ham  The guy did some bitching but I acted like i'd...  False\n",
       "5571   ham                         Rofl. Its true to its name  False\n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b178b608",
   "metadata": {},
   "source": [
    "So now we dont need type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03f7cecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                            message   spam\n",
       "0   ham  Go until jurong point, crazy.. Available only ...  False\n",
       "1   ham                      Ok lar... Joking wif u oni...  False\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   True\n",
       "3   ham  U dun say so early hor... U c already then say...  False\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...  False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9eefec",
   "metadata": {},
   "source": [
    "It didn't drop correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4efee6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message   spam\n",
       "0  Go until jurong point, crazy.. Available only ...  False\n",
       "1                      Ok lar... Joking wif u oni...  False\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   True\n",
       "3  U dun say so early hor... U c already then say...  False\n",
       "4  Nah I don't think he goes to usf, he lives aro...  False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95175615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of SPAM messages:\n",
      "747\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of SPAM messages:\")\n",
    "print(len(df[df[\"spam\"]==True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d0894c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Not SPAM messages:\n",
      "4825\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Not SPAM messages:\")\n",
    "print(len(df[df[\"spam\"]==False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd510b",
   "metadata": {},
   "source": [
    "Lets learn how CountVectorizer works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98032178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing' 'hello' 'is' 'mars' 'perfect' 'today']\n",
      "=======\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 9 stored elements and shape (2, 6)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t3\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "=======\n",
      "[[1 3 1 0 0 1]\n",
      " [0 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=6)\n",
    "documents = [\n",
    "    \"Hello world. Today is amazing. Hello hello\",\n",
    "    \"Hello mars, today is perfect\"\n",
    "]\n",
    "cv.fit(documents)\n",
    "print(cv.get_feature_names_out())\n",
    "out = cv.transform(documents)\n",
    "print(\"=======\")\n",
    "print(out)\n",
    "print(\"=======\")\n",
    "print(out.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd47f61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing' 'hello' 'is' 'mars' 'perfect' 'today' 'world']\n",
      "=======\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 10 stored elements and shape (2, 7)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t3\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "=======\n",
      "[[1 3 1 0 0 1 1]\n",
      " [0 1 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_features=7) # Change this from 1 to 7.  even if you increase beyond 7, it will not change because there are only 7 unique words here\n",
    "documents = [\n",
    "    \"Hello world. Today is amazing. Hello hello\",\n",
    "    \"Hello mars, today is perfect\"\n",
    "]\n",
    "cv.fit(documents)\n",
    "print(cv.get_feature_names_out())\n",
    "out = cv.transform(documents)\n",
    "print(\"=======\")\n",
    "print(out)\n",
    "print(\"=======\")\n",
    "print(out.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536d99d9",
   "metadata": {},
   "source": [
    "Lets apply this CountVectorizer to our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8159293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 12 stored elements and shape (1, 1000)>\n",
      "  Coords\tValues\n",
      "  (0, 349)\t1\n",
      "  (0, 887)\t1\n",
      "  (0, 661)\t1\n",
      "  (0, 206)\t1\n",
      "  (0, 90)\t1\n",
      "  (0, 613)\t1\n",
      "  (0, 429)\t1\n",
      "  (0, 361)\t1\n",
      "  (0, 973)\t1\n",
      "  (0, 830)\t1\n",
      "  (0, 358)\t1\n",
      "  (0, 921)\t1\n",
      "-------\n",
      "up\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_features=1000)\n",
    "messages = cv.fit_transform(df[\"message\"])\n",
    "print(messages[0, :])\n",
    "print(\"-------\")\n",
    "print(cv.get_feature_names_out()[888])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0ab70",
   "metadata": {},
   "source": [
    "There was an alternative for CountVectorizer. i.e, TFidfVectorizer Which gives slightly better results. \n",
    "Just use this Tfidf inplace of cv. no big change here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa3dfed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 12 stored elements and shape (1, 1000)>\n",
      "  Coords\tValues\n",
      "  (0, 349)\t0.21767405036534812\n",
      "  (0, 887)\t0.3385917224021876\n",
      "  (0, 661)\t0.37588329053830144\n",
      "  (0, 206)\t0.3721714491861035\n",
      "  (0, 90)\t0.3594536477277268\n",
      "  (0, 613)\t0.22967925385076954\n",
      "  (0, 429)\t0.15749629055186737\n",
      "  (0, 361)\t0.26546931912738453\n",
      "  (0, 973)\t0.3250709155826965\n",
      "  (0, 830)\t0.2289232644503655\n",
      "  (0, 358)\t0.22529489923658547\n",
      "  (0, 921)\t0.26851543603096684\n",
      "up\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "messages = vectorizer.fit_transform(df[\"message\"])\n",
    "print(messages[0, :])\n",
    "print(vectorizer.get_feature_names_out()[888])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc037c5a",
   "metadata": {},
   "source": [
    "Now lets train the neuron. Lets start with what variables we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a15d838",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse array length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m cv = CountVectorizer(max_features=\u001b[32m1000\u001b[39m)\n\u001b[32m     14\u001b[39m messages = cv.fit_transform(df[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m X = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(X)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# y = torch.tensor(df[\"spam\"], dtype=torch.float32)\\\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#         .reshape((-1, 1))\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\env\\Lib\\site-packages\\scipy\\sparse\\_base.py:449\u001b[39m, in \u001b[36m_spbase.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msparse array length is ambiguous; use getnnz()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33m or shape[0]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: sparse array length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "cv = CountVectorizer(max_features=1000)\n",
    "messages = cv.fit_transform(df[\"message\"])\n",
    "\n",
    "X = torch.tensor(messages, dtype=torch.float32)\n",
    "print(X)\n",
    "# y = torch.tensor(df[\"spam\"], dtype=torch.float32)\\\n",
    "#         .reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c408c78",
   "metadata": {},
   "source": [
    "X was giving some errors, the error says it is a sparse matrix while converting it into tensor. We need to convert it into dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "409e5df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([5572, 1000])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(messages.todense(), dtype=torch.float32)\n",
    "# y = torch.tensor(df[\"spam\"], dtype=torch.float32)\\\n",
    "#         .reshape((-1, 1))\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c2d8b",
   "metadata": {},
   "source": [
    "lets see y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4ce1c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1.,  ..., 0., 0., 0.])\n",
      "torch.Size([5572])\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor(df[\"spam\"], dtype=torch.float32)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b367395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        ...,\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "torch.Size([5572, 1])\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor(df[\"spam\"], dtype=torch.float32).reshape((-1,1))\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be628cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1566, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1304, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1157, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1061, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0989, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0881, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0837, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0799, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0764, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0733, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0705, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0679, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0656, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0635, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0616, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0582, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0567, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0553, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0541, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0507, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0498, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0489, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0473, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0465, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0458, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0452, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0445, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0439, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0434, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0428, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0423, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0419, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0414, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0410, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0405, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0401, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0397, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0394, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0390, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0387, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0383, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0380, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0377, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0374, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0371, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0369, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0366, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0363, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0361, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0359, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0356, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0354, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0352, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0350, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0347, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0345, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0343, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0342, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0338, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0336, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0334, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0333, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0331, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0329, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0328, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0326, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0325, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0323, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0322, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0320, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0319, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0317, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0316, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0315, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0313, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0312, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0311, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0310, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0309, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0307, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0306, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0305, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0304, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0303, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0302, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0301, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0300, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0299, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0298, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0297, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0296, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0295, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0294, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0293, grad_fn=<MseLossBackward0>)\n",
      "tensor([[-0.0526],\n",
      "        [ 0.0159],\n",
      "        [ 0.9105],\n",
      "        ...,\n",
      "        [-0.0266],\n",
      "        [ 0.2122],\n",
      "        [ 0.0395]])\n",
      "tensor(-0.6430)\n",
      "tensor(1.4552)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "cv = CountVectorizer(max_features=1000)\n",
    "messages = cv.fit_transform(df[\"message\"])\n",
    "\n",
    "X = torch.tensor(messages.todense(), dtype=torch.float32)\n",
    "y = torch.tensor(df[\"spam\"], dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "model = nn.Linear(1000, 1)  # our X has 1 thousand entries as input and we need to get only 1 output\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = loss_fn(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0: \n",
    "        print(loss)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "    print(y_pred)\n",
    "    print(y_pred.min())\n",
    "    print(y_pred.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0d1ee9",
   "metadata": {},
   "source": [
    "In this use case we should not get negative values of probability. So we need to use a Sigmoid function. Which only gives 0 or 1 as output. Sigmoid example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2185ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93ec7094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.8808, 0.9526])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.sigmoid(torch.tensor([-100, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "015d9905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a12474b740>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = torch.arange(-5, 5, step=0.1)\n",
    "y = nn.functional.sigmoid(X)\n",
    "plt.plot(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685fd2bf",
   "metadata": {},
   "source": [
    "We are changing only the loss function not the neuron. Keep in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3211adaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7010, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5406, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4668, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4237, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.3938, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.3708, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.3519, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.3356, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.3213, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.3086, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2971, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2866, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2772, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2685, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2605, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2531, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2463, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2400, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2342, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2287, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2236, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2188, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2143, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2101, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2061, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2024, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1988, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1954, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1922, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1892, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1863, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1836, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1809, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1784, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1760, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1737, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1715, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1693, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1673, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1653, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1635, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1616, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1599, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1582, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1566, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1550, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1535, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1520, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1505, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1492, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1478, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1465, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1453, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1440, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1428, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1417, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1405, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1394, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1384, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1373, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1363, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1353, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1344, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1334, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1325, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1316, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1308, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1299, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1291, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1283, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1275, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1267, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1259, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1252, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1244, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1237, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1230, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1223, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1217, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1210, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1204, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1197, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1191, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1185, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1179, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1173, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1167, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1162, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1156, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1151, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1145, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1140, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1135, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1130, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1125, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1120, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1115, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1110, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1105, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1101, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor([[-3.7014],\n",
      "        [-3.2589],\n",
      "        [ 2.7535],\n",
      "        ...,\n",
      "        [-3.6742],\n",
      "        [-2.8637],\n",
      "        [-2.5724]])\n",
      "tensor(-17.1047)\n",
      "tensor(7.3134)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "cv = CountVectorizer(max_features=1000)\n",
    "messages = cv.fit_transform(df[\"message\"])\n",
    "\n",
    "X = torch.tensor(messages.todense(), dtype=torch.float32)\n",
    "y = torch.tensor(df[\"spam\"], dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "model = nn.Linear(1000, 1)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = loss_fn(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(loss)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "    print(y_pred)\n",
    "    print(y_pred.min())\n",
    "    print(y_pred.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db34ec4",
   "metadata": {},
   "source": [
    "Still the output contains the negative values. increasing learning rate from 0.01 to 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "634ea06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6967, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2245, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1639, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1366, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1205, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1097, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1018, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0957, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0908, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0867, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor([[0.0126],\n",
      "        [0.0212],\n",
      "        [0.9727],\n",
      "        ...,\n",
      "        [0.0138],\n",
      "        [0.0394],\n",
      "        [0.0419]])\n",
      "tensor(8.4896e-11)\n",
      "tensor(0.9999)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(1000, 1)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = loss_fn(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        print(loss)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = nn.functional.sigmoid(model(X)) # model will be predicted and sent to sigmoid function to get the values from 0 to 1 only\n",
    "    print(y_pred)\n",
    "    print(y_pred.min())\n",
    "    print(y_pred.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c781b133",
   "metadata": {},
   "source": [
    "Now how to evaluate the model with performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f70e7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6908, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2249, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1641, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1367, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1206, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1098, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1018, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0957, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0908, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0867, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "accuracy: tensor(0.9799)\n",
      "sensitivity: tensor(0.9224)\n",
      "specificity: tensor(0.9888)\n",
      "precision: tensor(0.9273)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "cv = CountVectorizer(max_features=1000)\n",
    "messages = cv.fit_transform(df[\"message\"])\n",
    "\n",
    "X = torch.tensor(messages.todense(), dtype=torch.float32)\n",
    "y = torch.tensor(df[\"spam\"], dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "model = nn.Linear(1000, 1)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = loss_fn(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        print(loss)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = nn.functional.sigmoid(model(X)) > 0.25\n",
    "    print(\"accuracy:\", (y_pred == y)\\\n",
    "        .type(torch.float32).mean())\n",
    "    \n",
    "    print(\"sensitivity:\", (y_pred[y == 1] == y[y == 1])\\\n",
    "        .type(torch.float32).mean())\n",
    "    \n",
    "    print(\"specificity:\", (y_pred[y == 0] == y[y == 0])\\\n",
    "        .type(torch.float32).mean())\n",
    "\n",
    "    print(\"precision:\", (y_pred[y_pred == 1] == y[y_pred == 1])\\\n",
    "        .type(torch.float32).mean()) \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba473a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1341)\n",
      "tensor(0.8659)\n",
      "======\n",
      "tensor(0.1333)\n",
      "tensor(0.8667)\n"
     ]
    }
   ],
   "source": [
    "print((y == 1).type(torch.float32).mean())\n",
    "print((y == 0).type(torch.float32).mean())\n",
    "print(\"======\")\n",
    "print((y_pred == 1).type(torch.float32).mean())\n",
    "print((y_pred == 0).type(torch.float32).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76d271d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(0.)\n",
      "======\n",
      "tensor(0.9273)\n",
      "tensor(0.0120)\n"
     ]
    }
   ],
   "source": [
    "print((y[y == 1]).type(torch.float32).mean())\n",
    "print((y[y == 0]).type(torch.float32).mean())\n",
    "print(\"======\")\n",
    "print((y[y_pred == 1]).type(torch.float32).mean())\n",
    "print((y[y_pred == 0]).type(torch.float32).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7edf1e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9224)\n",
      "tensor(0.0112)\n",
      "======\n",
      "tensor(1.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print((y_pred[y == 1]).type(torch.float32).mean())\n",
    "print((y_pred[y == 0]).type(torch.float32).mean())\n",
    "print(\"======\")\n",
    "print((y_pred[y_pred == 1]).type(torch.float32).mean())\n",
    "print((y_pred[y_pred == 0]).type(torch.float32).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25214f5",
   "metadata": {},
   "source": [
    "Let's train the model with training data and do the validation with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34d7e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "df_train = df.sample(frac=0.8, random_state=0)\n",
    "df_val = df.drop(index=df_train.index)\n",
    "\n",
    "cv = CountVectorizer(max_features=5000)\n",
    "messages_train = cv.fit_transform(df_train[\"message\"])\n",
    "messages_val = cv.transform(df_val[\"message\"])\n",
    "\n",
    "X_train = torch.tensor(messages_train.todense(), dtype=torch.float32)\n",
    "y_train = torch.tensor(df_train[\"spam\"].values, dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "X_val = torch.tensor(messages_val.todense(), dtype=torch.float32)\n",
    "y_val = torch.tensor(df_val[\"spam\"].values, dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "model = nn.Linear(5000, 1)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79759246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X, y): \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = nn.functional.sigmoid(model(X)) > 0.25\n",
    "        print(\"accuracy:\", (y_pred == y)\\\n",
    "            .type(torch.float32).mean())\n",
    "        \n",
    "        print(\"sensitivity:\", (y_pred[y == 1] == y[y == 1])\\\n",
    "            .type(torch.float32).mean())\n",
    "        \n",
    "        print(\"specificity:\", (y_pred[y == 0] == y[y == 0])\\\n",
    "            .type(torch.float32).mean())\n",
    "\n",
    "        print(\"precision:\", (y_pred[y_pred == 1] == y[y_pred == 1])\\\n",
    "            .type(torch.float32).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14ee3bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6970, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Evaluating on the training data\n",
      "accuracy: tensor(0.9643)\n",
      "sensitivity: tensor(0.8651)\n",
      "specificity: tensor(0.9800)\n",
      "precision: tensor(0.8723)\n",
      "Evaluating on the validation data\n",
      "accuracy: tensor(0.9614)\n",
      "sensitivity: tensor(0.8561)\n",
      "specificity: tensor(0.9764)\n",
      "precision: tensor(0.8380)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 1000): # Change from 1000, 5000, to 10000 and see the differen\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        print(loss) \n",
    "        \n",
    "print(\"Evaluating on the training data\")\n",
    "evaluate_model(X_train, y_train)\n",
    "\n",
    "print(\"Evaluating on the validation data\")\n",
    "evaluate_model(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8281f7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2219, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1606, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1327, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1161, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1048, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Evaluating on the training data\n",
      "accuracy: tensor(0.9773)\n",
      "sensitivity: tensor(0.9178)\n",
      "specificity: tensor(0.9868)\n",
      "precision: tensor(0.9163)\n",
      "Evaluating on the validation data\n",
      "accuracy: tensor(0.9749)\n",
      "sensitivity: tensor(0.9209)\n",
      "specificity: tensor(0.9826)\n",
      "precision: tensor(0.8828)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5000): # Change from 1000, 5000, to 10000 and see the differen\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        print(loss) \n",
    "        \n",
    "print(\"Evaluating on the training data\")\n",
    "evaluate_model(X_train, y_train)\n",
    "\n",
    "print(\"Evaluating on the validation data\")\n",
    "evaluate_model(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c49e443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0965, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0901, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0848, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0804, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0767, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0735, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0706, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0681, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0658, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0637, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Evaluating on the training data\n",
      "accuracy: tensor(0.9838)\n",
      "sensitivity: tensor(0.9375)\n",
      "specificity: tensor(0.9912)\n",
      "precision: tensor(0.9437)\n",
      "Evaluating on the validation data\n",
      "accuracy: tensor(0.9785)\n",
      "sensitivity: tensor(0.9209)\n",
      "specificity: tensor(0.9867)\n",
      "precision: tensor(0.9078)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10000): # Change from 1000, 5000, to 10000 and see the differen\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)\n",
    "\n",
    "print(\"Evaluating on the training data\")\n",
    "evaluate_model(X_train, y_train)\n",
    "\n",
    "print(\"Evaluating on the validation data\")\n",
    "evaluate_model(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a0a60",
   "metadata": {},
   "source": [
    "If you observe the above one, it has a good precision than others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8386dce",
   "metadata": {},
   "source": [
    "Now the model is ready for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4793ba3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7006, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2237, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1632, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1359, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1198, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1089, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1009, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0947, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0897, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0856, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Evaluating on the training data\n",
      "accuracy: tensor(0.9791)\n",
      "sensitivity: tensor(0.9211)\n",
      "specificity: tensor(0.9883)\n",
      "precision: tensor(0.9256)\n",
      "Evaluating on the validation data\n",
      "accuracy: tensor(0.9740)\n",
      "sensitivity: tensor(0.9137)\n",
      "specificity: tensor(0.9826)\n",
      "precision: tensor(0.8819)\n",
      "tensor([[0.0517],\n",
      "        [0.6089],\n",
      "        [0.0129]])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "df_train = df.sample(frac=0.8, random_state=0)\n",
    "df_val = df.drop(index=df_train.index)\n",
    "\n",
    "cv = CountVectorizer(max_features=1000)\n",
    "messages_train = cv.fit_transform(df_train[\"message\"])\n",
    "messages_val = cv.transform(df_val[\"message\"])\n",
    "\n",
    "X_train = torch.tensor(messages_train.todense(), dtype=torch.float32)\n",
    "y_train = torch.tensor(df_train[\"spam\"].values, dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "X_val = torch.tensor(messages_val.todense(), dtype=torch.float32)\n",
    "y_val = torch.tensor(df_val[\"spam\"].values, dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "model = nn.Linear(1000, 1)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        print(loss)\n",
    "\n",
    "def evaluate_model(X, y): \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = nn.functional.sigmoid(model(X)) > 0.25\n",
    "        print(\"accuracy:\", (y_pred == y)\\\n",
    "            .type(torch.float32).mean())\n",
    "        \n",
    "        print(\"sensitivity:\", (y_pred[y == 1] == y[y == 1])\\\n",
    "            .type(torch.float32).mean())\n",
    "        \n",
    "        print(\"specificity:\", (y_pred[y == 0] == y[y == 0])\\\n",
    "            .type(torch.float32).mean())\n",
    "\n",
    "        print(\"precision:\", (y_pred[y_pred == 1] == y[y_pred == 1])\\\n",
    "            .type(torch.float32).mean()) \n",
    "        \n",
    "print(\"Evaluating on the training data\")\n",
    "evaluate_model(X_train, y_train)\n",
    "\n",
    "print(\"Evaluating on the validation data\")\n",
    "evaluate_model(X_val, y_val)\n",
    "\n",
    "custom_messages = cv.transform([\n",
    "    \"We have release a new product, do you want to buy it?\", \n",
    "    \"Winner! Great deal, call us to get this product for free\",\n",
    "    \"Tomorrow is my birthday, do you come to the party?\"\n",
    "])\n",
    "\n",
    "X_custom = torch.tensor(custom_messages.todense(), dtype=torch.float32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = nn.functional.sigmoid(model(X_custom))\n",
    "    print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b120058",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47426f95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Another way using transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2626d3f",
   "metadata": {},
   "source": [
    "Normal way to generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1319195d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab72133f4d8494bae2769d870c18685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f05872101642528da71f23ab5de269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380436a93130456d87c43ac24050a1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e14eecd8993451096b1616996b1c7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 170, 33, 800, 10, 92, 1152, 6, 109, 47, 236, 7, 907, 24, 116, 2], [0, 46722, 328, 2860, 432, 6, 486, 201, 7, 120, 42, 1152, 13, 481, 2], [0, 38849, 16, 127, 4115, 6, 109, 47, 283, 7, 5, 537, 116, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "messages = [\n",
    "    \"We have release a new product, do you want to buy it?\", \n",
    "    \"Winner! Great deal, call us to get this product for free\",\n",
    "    \"Tomorrow is my birthday, do you come to the party?\",\n",
    "]\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "out = tokenizer(messages)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e89ea943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 170, 33, 800, 10, 92, 1152, 6, 109, 47, 236, 7, 907, 24, 116, 2], [0, 46722, 328, 2860, 432, 6, 486, 201, 7, 120, 42, 1152, 13, 481, 2, 1], [0, 38849, 16, 127, 4115, 6, 109, 47, 283, 7, 5, 537, 116, 2, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# Observe the padding difference\n",
    "out = tokenizer(messages, padding=True)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a47b8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 170, 33, 800, 10, 92, 1152, 6, 109, 47, 236, 7, 907, 24, 116, 2], [0, 46722, 328, 2860, 432, 6, 486, 201, 7, 120, 42, 1152, 13, 481, 2, 1], [0, 38849, 16, 127, 4115, 6, 109, 47, 283, 7, 5, 537, 116, 2, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# Now it says to process only 512 tokens to process. Only 512 tokens not 512 characters. If it exceed 512 tokens do the truncation\n",
    "out = tokenizer(messages, \n",
    "                padding=True, \n",
    "                max_length=512, \n",
    "                truncation=True) # \n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "941a9fb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5de70bcff1c40d0ad28123d68a9e012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   170,    33,   800,    10,    92,  1152,     6,   109,    47,\n",
      "           236,     7,   907,    24,   116,     2],\n",
      "        [    0, 46722,   328,  2860,   432,     6,   486,   201,     7,   120,\n",
      "            42,  1152,    13,   481,     2,     1],\n",
      "        [    0, 38849,    16,   127,  4115,     6,   109,    47,   283,     7,\n",
      "             5,   537,   116,     2,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n",
      "torch.Size([3, 16, 768])\n",
      "torch.Size([3, 768])\n",
      "tensor([-4.1475e-01, -9.5950e-01, -1.9372e+00, -1.8202e+00, -1.3529e+00,\n",
      "        -3.2617e-01, -4.3142e-02, -2.8496e-01, -4.4574e-01, -2.8201e+00,\n",
      "        -7.7632e-01,  9.4289e-01,  3.7461e-01,  3.3576e-01,  5.2033e-01,\n",
      "         7.1250e-01, -9.4961e-01,  7.0305e-01, -3.9577e-01, -1.0670e+00,\n",
      "         2.0777e-01, -1.6843e-01, -1.0866e+00,  8.7861e-01,  3.0825e-01,\n",
      "        -8.2243e-01,  5.6214e-01,  2.9970e+00,  3.1303e-01, -3.6227e+00,\n",
      "         6.2353e-01,  3.4955e-01,  1.5166e-01, -9.7267e-01, -1.5525e+00,\n",
      "         1.6685e+00, -1.1181e-01,  1.4668e+00,  1.0774e-01, -2.3462e-01,\n",
      "         3.8726e-01,  1.2186e+00,  4.9061e-01, -4.8987e-01,  3.9203e-01,\n",
      "         4.2587e-01, -1.9299e+00,  5.9904e-01, -6.1466e-01,  7.6263e-01,\n",
      "        -1.3223e+00,  6.8178e-01, -8.5020e-01,  1.1885e-01, -6.4347e-02,\n",
      "        -1.0802e+00,  3.9521e-01, -1.3714e-02, -1.4325e-01,  3.8025e-01,\n",
      "         2.1685e-01,  7.3130e-01,  3.3559e-01,  6.3142e-01, -7.0818e-01,\n",
      "        -1.4830e-01,  7.6586e-01,  5.2708e-01,  9.9074e-01,  1.5059e-01,\n",
      "         3.7120e-01, -2.2960e+00, -5.4099e-01,  6.0844e-02, -6.0287e-01,\n",
      "         4.0135e-01, -1.3555e+00, -8.2851e-01,  5.8254e-02, -5.3967e-01,\n",
      "         3.3954e-01, -4.1404e-01,  7.1217e-01,  7.1192e-02,  3.3282e-01,\n",
      "        -8.0114e-01, -1.8887e-01,  1.0675e-01,  6.4966e-01,  7.1148e-02,\n",
      "        -5.4697e-01,  2.0469e-01,  1.0316e+00, -2.8317e-01,  7.7322e-01,\n",
      "         1.8169e+00,  1.3140e-01, -1.0399e+00, -1.5442e-01, -1.8674e-01,\n",
      "         1.1935e+00,  8.4956e-01, -1.1487e+00,  1.8479e-01,  1.7713e+00,\n",
      "         2.9089e-01,  1.0326e+00,  5.9302e-01,  4.7164e-01,  2.2687e-01,\n",
      "        -1.4431e+00,  2.6868e-01,  6.6778e-01, -1.5047e-01, -7.7921e-01,\n",
      "        -4.5424e-01, -1.4516e+00, -7.8665e-01,  2.0442e+00, -4.3260e-01,\n",
      "         2.0610e+00,  7.0859e-01, -7.7400e-01,  7.4771e-01,  1.1789e-01,\n",
      "        -2.6061e-02, -1.0395e+00,  7.3904e-01, -1.1985e+00, -1.8468e-01,\n",
      "         8.6259e-01,  1.5406e+00,  1.3009e+00, -9.2566e-02,  2.5857e-01,\n",
      "        -1.3698e+00,  4.3143e-01,  9.4271e-01,  9.5243e-01, -3.1455e-01,\n",
      "         1.1327e-01, -8.5425e-01, -2.0648e-01, -9.7961e-01,  1.7658e-01,\n",
      "         2.2374e-01, -9.8801e-02, -1.4849e-01,  3.2218e-01,  7.8808e-01,\n",
      "         2.5684e-01,  7.6109e-01,  6.6792e-01,  4.9621e-01, -4.2776e-02,\n",
      "         7.5357e-01, -3.7807e-01,  9.1128e-01,  8.9477e-01, -2.5515e-01,\n",
      "        -6.1027e-01,  1.0094e-01, -4.0385e-01,  5.1562e-02, -2.6430e-01,\n",
      "        -5.6223e-01,  4.2568e-01,  6.4352e-01,  8.1615e-01,  2.2419e-01,\n",
      "        -6.2836e-02,  7.7046e-01, -1.1277e+00,  9.8505e-01, -2.1810e-01,\n",
      "        -5.7274e-01, -6.7072e-01, -5.3110e-01,  1.3540e-01,  6.0107e-01,\n",
      "         4.3962e-01,  1.8937e-01,  4.5798e-01, -1.1746e+00,  4.9404e-01,\n",
      "         7.9294e-01,  7.9963e-01,  5.7125e-01, -7.7112e-01, -1.0891e+00,\n",
      "         6.6052e-01,  3.8090e-01,  1.8508e-01, -8.2274e-01, -1.5393e-01,\n",
      "         3.9580e-01, -3.0838e-01,  1.3217e+00,  1.0522e+00,  1.8827e-01,\n",
      "         1.3554e+00, -1.1751e-01,  6.2035e-01,  5.5142e-01,  1.9135e+00,\n",
      "        -5.8111e-01,  6.9197e-01, -2.2210e-01, -3.9427e-01,  4.9954e-01,\n",
      "        -4.7491e-01, -5.1088e-02,  9.0249e-01,  1.0499e+00, -7.2501e-01,\n",
      "         6.9398e-02, -1.3099e-01, -8.0542e-01,  1.7105e+00, -1.4883e+00,\n",
      "         3.4443e-01, -1.1071e+00,  1.1600e+00,  2.1013e-01,  1.1625e+00,\n",
      "         3.8705e-01, -5.8039e-01,  1.7525e+00,  3.4043e-01,  4.5692e-01,\n",
      "         5.6630e-01, -2.2989e-02, -1.4226e-01, -3.5858e-01, -2.0539e-01,\n",
      "         1.0087e+00, -1.2032e+00,  1.1797e+00, -1.4366e+00, -5.3436e-01,\n",
      "         1.1397e-01,  2.2150e+00,  9.5714e-01,  1.6787e+00, -7.7719e-01,\n",
      "         8.8115e-01,  2.6390e-01, -3.4971e-01,  9.6814e-01,  7.4955e-01,\n",
      "        -5.3866e-01,  5.1850e-01, -1.3745e+00, -5.3499e-01,  2.8552e-01,\n",
      "        -6.2070e-01,  3.9283e-01,  7.1960e-01, -6.6273e-02,  6.3013e-01,\n",
      "        -7.6092e-01,  1.2916e-01, -5.2718e-01, -4.8940e-01,  5.4991e-01,\n",
      "         3.2553e-01, -2.4546e-01,  1.0580e+00,  2.0010e-02, -8.6345e-01,\n",
      "        -9.9340e-01,  5.9759e-02,  2.5060e-01, -4.0710e-01,  7.3701e-01,\n",
      "        -3.7636e-01,  9.4703e-02, -2.5705e-01, -4.8306e-01,  1.2695e+00,\n",
      "         1.3508e+00,  1.2664e+00,  9.7701e-01, -2.0097e+00,  2.3033e-01,\n",
      "        -2.6531e-01,  4.9801e-01,  5.6048e-01,  4.4080e-02, -2.2131e-01,\n",
      "         1.6945e+00,  1.3591e+00, -1.1824e+00, -3.9429e-01,  1.4459e+00,\n",
      "        -5.0818e-01, -1.2323e+00,  1.8119e-01, -6.0612e-01,  1.4180e-01,\n",
      "        -1.2813e+00,  8.3170e-01, -8.5896e-03,  1.0103e+00, -6.4637e-02,\n",
      "         2.0447e-01, -5.9049e-02, -8.4107e-02, -4.2917e-01, -8.7000e-01,\n",
      "         8.8375e-01,  3.1787e-01, -4.0937e-01, -5.6909e-02,  1.3654e+00,\n",
      "         1.1932e+00,  3.2359e-01, -4.6136e-01, -2.2596e-01,  7.1681e-02,\n",
      "         9.9944e-01,  5.0000e-01, -3.4226e-01, -1.3771e-01, -3.8507e-01,\n",
      "        -2.3807e-02,  4.7299e-01, -1.5213e-01,  7.3155e-01,  1.5441e+00,\n",
      "        -3.1469e-01,  5.9238e-01,  4.2674e+00,  4.1413e-01,  1.6818e-01,\n",
      "        -1.2494e+00,  2.7738e-01,  7.4282e-02,  2.3215e-01,  3.6279e-02,\n",
      "         7.7495e-01, -2.1801e-01, -8.6731e-01,  6.0887e-01,  1.3523e+00,\n",
      "         8.0288e-01,  7.0077e-01,  1.7358e-01, -7.6068e-01, -1.6251e-01,\n",
      "        -7.6872e-01, -1.2867e+00,  8.9312e-03, -4.8212e-01,  1.5390e+00,\n",
      "        -5.8121e-01,  3.0819e-01, -5.9069e-01,  1.6313e+00,  8.9079e-01,\n",
      "         1.0093e-01, -8.9053e-01,  6.9939e-02,  2.0636e-01, -7.1217e-01,\n",
      "         2.7813e-03, -1.2153e-01,  7.6926e-01, -5.6563e-01, -2.2855e-01,\n",
      "        -1.1961e+00,  4.1717e-01,  7.0554e-01, -1.5196e+00, -1.6051e+00,\n",
      "        -1.6529e-01,  9.7834e-01, -1.5735e+00,  6.2087e-01,  6.3398e-01,\n",
      "        -6.1600e-01,  1.2622e-01,  8.4936e-02,  1.9111e-01,  1.7907e-01,\n",
      "         1.9763e+00,  1.0605e+00, -9.5546e-01, -2.1608e+00,  7.2820e-01,\n",
      "         9.9788e-01,  1.5679e-01,  1.1343e+00,  1.2692e-01,  8.6366e-01,\n",
      "         5.6870e-01, -1.1499e-02,  1.4351e+00,  4.6019e-01,  9.9197e-01,\n",
      "         1.5703e+00, -1.6681e-01,  1.3047e+00,  1.6867e-01, -1.2434e+00,\n",
      "         2.9224e+00, -2.7052e-02,  5.7483e-01,  6.5370e-01,  3.1998e-01,\n",
      "        -5.6605e-01,  2.2950e-01,  3.5763e-01,  1.4066e-01,  1.0723e+00,\n",
      "         1.4007e-01, -3.0236e-01,  2.3099e-01, -1.4623e+00, -7.5033e-01,\n",
      "         2.0951e-01, -2.6282e-01, -3.1380e-01, -1.7540e+00,  4.8429e-01,\n",
      "        -2.7652e-01,  5.0351e-01,  3.2027e-01, -2.9001e-01,  3.3029e-01,\n",
      "         3.2933e-01,  2.9229e-02,  3.7104e-01,  6.2425e-01,  2.7658e-01,\n",
      "         3.2014e-01,  5.0463e-01, -1.6570e+00, -4.1505e-01,  8.7000e-01,\n",
      "        -1.3688e+00, -2.6003e-01,  8.1837e-01, -6.3115e-01, -2.5408e-01,\n",
      "        -6.1117e-01,  1.8765e-01,  5.9824e-01,  4.5026e-01,  1.0128e+00,\n",
      "        -1.7212e-01,  1.9344e-01,  3.0133e-01,  6.9407e-01,  3.4426e-01,\n",
      "         7.7636e-01, -6.8282e-01, -2.7234e-01,  2.6596e-01, -1.3303e+00,\n",
      "        -7.2120e-01,  1.6488e-01,  1.8021e-02,  3.7510e-01, -4.0045e-01,\n",
      "        -9.2344e-02,  1.8156e-02,  1.3516e-01,  7.6951e-01,  3.3846e-01,\n",
      "        -1.2066e-01, -3.6541e-01, -4.0758e-01, -2.2844e-03, -5.8193e-01,\n",
      "         6.5466e-01,  5.0772e-01, -2.9894e-01,  2.2297e-01, -6.9664e-01,\n",
      "         4.4005e-01,  7.0805e-01, -2.1537e-01,  1.0084e+00, -7.6860e-01,\n",
      "         9.7583e-01, -3.3504e-01, -6.4838e-01,  1.7211e+00,  1.0415e+00,\n",
      "         5.8568e-01,  2.9294e-01, -2.9926e-01, -9.1095e-02,  2.0280e-01,\n",
      "         6.9967e-01, -1.1933e-01, -9.6195e+00,  3.7097e-01,  9.4654e-01,\n",
      "         3.2910e-01, -4.5671e-02,  2.1232e-01, -3.5392e-01,  1.0385e+00,\n",
      "         4.4189e-01,  1.4281e+00,  5.5003e-01,  9.7709e-01,  8.6210e-01,\n",
      "         1.1561e-01,  1.4830e+00,  1.7212e+00,  4.9958e-03, -2.7197e-01,\n",
      "        -4.3733e-01,  1.1509e-01, -1.2504e-01, -6.5717e-01,  9.6498e-01,\n",
      "        -1.0891e-01, -1.1341e+00, -3.5492e-01, -1.0765e+00, -1.3831e-01,\n",
      "        -1.1789e-01,  1.2428e+00, -3.5665e-01,  3.3932e-01, -3.3820e-01,\n",
      "        -5.0656e-02, -1.0472e+00, -6.0158e-02, -1.2930e-01,  7.4063e-01,\n",
      "        -1.1751e-01, -2.0110e-01,  2.5657e-01, -3.1756e-01,  1.0400e+00,\n",
      "        -4.7883e-01,  6.1825e-03,  9.6114e-01, -7.0611e-01, -2.3108e-01,\n",
      "        -3.2789e-02, -7.3197e-02, -3.2422e-01,  6.7265e-01,  4.5614e-03,\n",
      "         1.6397e-01,  1.1572e-01, -3.4807e-01,  3.5522e-02, -7.6326e-01,\n",
      "        -9.8880e-01,  4.2391e-01, -1.0604e+00,  4.4391e-01,  7.3092e-01,\n",
      "        -2.7843e-01,  7.7305e-01,  1.0912e-02,  1.2804e-01, -1.9877e+00,\n",
      "        -4.1987e-01, -6.7401e-02,  1.6773e+00, -2.8985e-01,  8.0948e-02,\n",
      "         3.2138e-01,  1.2680e+00, -9.1868e-01, -4.4133e-01, -2.1895e+00,\n",
      "        -1.0830e+00,  1.1698e-01, -3.4170e-02, -7.0342e-01,  1.0932e+00,\n",
      "        -1.5132e+00,  4.7418e-01, -8.1649e-01,  6.9834e-01, -1.1079e+00,\n",
      "        -1.1196e+00,  1.2320e+00,  1.0540e-01, -4.2402e-01,  2.6490e-01,\n",
      "        -3.4179e-01,  1.2152e+00,  4.1531e-01, -1.7933e+00, -5.8631e-02,\n",
      "        -3.4828e-01,  1.9136e-01,  6.0939e-01, -5.8915e-01, -8.4627e-01,\n",
      "         3.8768e-01,  5.0225e-02, -1.5636e-01, -3.7990e-01, -1.4396e-01,\n",
      "        -4.3956e-01, -3.6645e-02,  3.9146e-01,  1.1234e+00, -1.5707e+00,\n",
      "        -1.3412e+00, -1.0952e+00,  1.8899e-01, -2.7015e+00,  6.2977e-01,\n",
      "        -1.4042e-01,  2.8649e-01,  6.3594e-01,  6.1251e-02,  2.9926e-01,\n",
      "         4.1174e-01, -2.5452e-01, -4.7790e-01, -1.4842e-01,  2.8987e-02,\n",
      "         1.7393e-01,  5.4744e-01,  4.4667e-02, -1.7993e-01,  7.4249e-01,\n",
      "        -2.9704e-01,  3.4378e-01, -2.6330e-01,  1.0131e+00, -8.9129e-01,\n",
      "        -2.3721e-02,  2.7531e-01,  1.8356e-01,  7.6636e-01, -1.7368e-01,\n",
      "         5.6646e-01,  1.2159e+00,  3.5576e-01,  2.0316e-01, -6.0108e-02,\n",
      "         7.1822e-01, -1.2945e-01, -3.2068e-01, -6.7184e-01,  7.5910e-01,\n",
      "         5.0962e-01,  8.8232e-01, -5.2475e-01, -5.9834e-01, -3.5976e-01,\n",
      "        -4.7668e-01, -3.8678e-01,  2.3469e-01,  3.0678e-01, -2.4897e-02,\n",
      "        -4.8672e-01,  1.2892e+00,  1.5046e+00, -6.7389e-01,  6.8612e-01,\n",
      "         2.4584e-02,  6.8452e-01,  5.7767e-01,  5.2325e-01,  1.9233e-01,\n",
      "         9.7426e-01,  4.7115e-02,  4.7954e-01,  4.0537e-02, -4.0768e-02,\n",
      "        -8.3592e-01,  7.0256e-01,  7.6637e-01,  5.6338e-01,  5.8097e-01,\n",
      "         8.0495e-01, -5.3367e-01,  4.8584e-01, -1.5728e-01, -9.4478e-01,\n",
      "         1.1926e+00,  5.5034e-01, -1.3355e+00, -5.9969e-01, -6.5493e-01,\n",
      "         2.4563e-01, -3.4546e-01,  1.6127e-01,  7.1841e-01, -9.3089e-01,\n",
      "         4.4267e-01,  4.5890e-01,  1.0778e+00,  2.2677e+00, -4.1448e-01,\n",
      "         4.8100e-01,  1.4988e+00,  9.3322e-01, -9.7011e-01,  2.9756e-01,\n",
      "         4.0918e-02,  3.2447e-01, -7.6698e-01,  4.3748e-01,  1.1181e+00,\n",
      "         1.0649e-01,  8.1863e-01, -8.4058e-01,  1.8906e-01,  8.4732e-01,\n",
      "         2.0598e+00, -3.5787e-01,  5.7401e-01, -3.6052e-01, -1.1555e+00,\n",
      "        -1.2010e-01,  5.5122e-01, -8.8001e-01, -4.8224e-01, -3.5766e-01,\n",
      "        -1.4321e+00,  3.6758e-01, -6.9261e-01,  2.8020e-01,  1.8237e-01,\n",
      "         7.8736e-01,  5.5919e-01, -1.9522e-01,  1.2480e+00,  1.9598e-01,\n",
      "         1.9535e+00, -6.4246e-01, -1.5658e-02, -2.7994e-01,  1.5930e-01,\n",
      "         1.7171e-01,  1.5765e+00,  9.6485e-01, -6.0650e-02,  3.7453e-01,\n",
      "         3.3513e-01,  7.8092e-01, -2.6718e-01, -5.5345e-01, -2.2483e-01,\n",
      "        -4.7379e-01, -3.8966e-03,  7.0742e-01,  6.7253e-01, -9.8426e-01,\n",
      "         5.1378e-01,  9.8561e-01,  5.6448e-01,  7.5835e-01, -2.9731e-01,\n",
      "        -8.3579e-01,  6.1841e-01, -2.7562e-02, -3.1357e-01,  8.2651e-02,\n",
      "        -3.6704e-03, -2.9639e-01, -2.1398e-02])\n"
     ]
    }
   ],
   "source": [
    "out = tokenizer(messages, \n",
    "                padding=True, \n",
    "                max_length=512, \n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\")\n",
    "\n",
    "bart_model = BartModel.from_pretrained(\"facebook/bart-base\")\n",
    "with torch.no_grad():\n",
    "    bart_model.eval()\n",
    "    print(out)\n",
    "    # print(*out)\n",
    "    # pred = bart_model(**out) # This sentence is same as below\n",
    "    pred = bart_model(\n",
    "        input_ids=out[\"input_ids\"], \n",
    "        attention_mask=out[\"attention_mask\"]\n",
    "    )\n",
    "    print(pred.last_hidden_state.shape) \n",
    "    embeddings = pred.last_hidden_state.mean(dim=1)\n",
    "    print(embeddings.shape)\n",
    "    print(embeddings[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a368a148",
   "metadata": {},
   "source": [
    "torch.Size([3, 16, 768])\n",
    "there are 3 dimensions here.\n",
    "our LLM predicted 1 token at a time.\n",
    "So it took 16 iterations for each sentence\n",
    "\n",
    "\n",
    "torch.Size([3, 16, 768]) Breakdown:\n",
    "\n",
    "3 — Batch size: You passed in a batch of 3 sequences (probably 3 sentences or documents).\n",
    "\n",
    "16 — Sequence length: Each input sequence is padded or truncated to a length of 16 tokens.\n",
    "\n",
    "768 — Hidden size: Each token in the sequence is represented by a 768-dimensional vector. This is a common hidden size for BART-base (similar to BERT-base).\n",
    "\n",
    "what is last_hidden_state?\n",
    "\n",
    "It's a tensor containing the final hidden layer's output for each token in each sequence.\n",
    "\n",
    "You can think of it as a 3D matrix:\\\n",
    "For each of the 3 input sequences →\\\n",
    "For each of the 16 tokens in the sequence →\\\n",
    "There's a 768-dimensional vector representing the model's internal understanding at that position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74b0263c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 20.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "messages = [\n",
    "    \"We have release a new product, do you want to buy it?\", \n",
    "    \"Winner! Great deal, call us to get this product for free\",\n",
    "    \"Tomorrow is my birthday, do you come to the party?\",\n",
    "]\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "bart_model = BartModel.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def convert_to_embeddings(messages):\n",
    "    embeddings_list = []\n",
    "    for message in tqdm(messages):\n",
    "        out = tokenizer([message], \n",
    "                        padding=True,\n",
    "                        max_length=512, \n",
    "                        truncation=True, \n",
    "                        return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bart_model.eval()\n",
    "\n",
    "            pred = bart_model(\n",
    "                input_ids=out[\"input_ids\"], \n",
    "                attention_mask=out[\"attention_mask\"]\n",
    "            )\n",
    "            embeddings = pred.last_hidden_state.mean(dim=1)\\\n",
    "                .reshape((-1)) # It is going to matrix ([1, 768]) to a vector([768]) or tensor, so it can be easily processed by the neuron \n",
    "            embeddings_list.append(embeddings)\n",
    "    return torch.stack(embeddings_list)\n",
    "X = convert_to_embeddings(messages)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da54aaa3",
   "metadata": {},
   "source": [
    "Now implement this code in the neuron. Here instead of Count Vectorizer we are using Bart model. The reason it works more accurate because it is working with meanings(LLM embeddings) not just the words(CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55e75903",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4458/4458 [02:18<00:00, 32.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1114/1114 [00:40<00:00, 27.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6647, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0277, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0205, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0168, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0144, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0126, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0113, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0102, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0094, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0086, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Evaluating on the training data\n",
      "accuracy: tensor(0.9984)\n",
      "sensitivity: tensor(0.9984)\n",
      "specificity: tensor(0.9984)\n",
      "precision: tensor(0.9902)\n",
      "Evaluating on the validation data\n",
      "accuracy: tensor(0.9946)\n",
      "sensitivity: tensor(0.9856)\n",
      "specificity: tensor(0.9959)\n",
      "precision: tensor(0.9716)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 31.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.6850e-03],\n",
      "        [8.0221e-01],\n",
      "        [4.5402e-07]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "bart_model = BartModel.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def convert_to_embeddings(messages):\n",
    "    embeddings_list = []\n",
    "    for message in tqdm(messages):\n",
    "        out = tokenizer([message], \n",
    "                        padding=True,\n",
    "                        max_length=512, \n",
    "                        truncation=True, \n",
    "                        return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bart_model.eval()\n",
    "\n",
    "            pred = bart_model(\n",
    "                input_ids=out[\"input_ids\"], \n",
    "                attention_mask=out[\"attention_mask\"]\n",
    "            )\n",
    "            embeddings = pred.last_hidden_state.mean(dim=1)\\\n",
    "                .reshape((-1))\n",
    "            embeddings_list.append(embeddings)\n",
    "    return torch.stack(embeddings_list)\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "df_train = df.sample(frac=0.8, random_state=0)\n",
    "df_val = df.drop(index=df_train.index)\n",
    "\n",
    "X_train = convert_to_embeddings(df_train[\"message\"].tolist())\n",
    "X_val = convert_to_embeddings(df_val[\"message\"].tolist())\n",
    "\n",
    "y_train = torch.tensor(df_train[\"spam\"].values, dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "y_val = torch.tensor(df_val[\"spam\"].values, dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "model = nn.Linear(768, 1)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        print(loss)\n",
    "\n",
    "def evaluate_model(X, y): \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = nn.functional.sigmoid(model(X)) > 0.25\n",
    "        print(\"accuracy:\", (y_pred == y)\\\n",
    "            .type(torch.float32).mean())\n",
    "        \n",
    "        print(\"sensitivity:\", (y_pred[y == 1] == y[y == 1])\\\n",
    "            .type(torch.float32).mean())\n",
    "        \n",
    "        print(\"specificity:\", (y_pred[y == 0] == y[y == 0])\\\n",
    "            .type(torch.float32).mean())\n",
    "\n",
    "        print(\"precision:\", (y_pred[y_pred == 1] == y[y_pred == 1])\\\n",
    "            .type(torch.float32).mean()) \n",
    "        \n",
    "print(\"Evaluating on the training data\")\n",
    "evaluate_model(X_train, y_train)\n",
    "\n",
    "print(\"Evaluating on the validation data\")\n",
    "evaluate_model(X_val, y_val)\n",
    "\n",
    "X_custom = convert_to_embeddings([\n",
    "    \"We have release a new product, do you want to buy it?\", \n",
    "    \"Winner! Great deal, call us to get this product for free\",\n",
    "    \"Tomorrow is my birthday, do you come to the party?\"\n",
    "])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = nn.functional.sigmoid(model(X_custom))\n",
    "    print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed0dc3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "476232ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and transforming training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4457/4457 [11:36<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and transforming validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1115/1115 [02:19<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.7211135029792786\n",
      "Iteration 1000, Loss: 0.035095930099487305\n",
      "Iteration 2000, Loss: 0.026426220312714577\n",
      "Iteration 3000, Loss: 0.02208225429058075\n",
      "Iteration 4000, Loss: 0.019298294559121132\n",
      "Iteration 5000, Loss: 0.017310665920376778\n",
      "Iteration 6000, Loss: 0.01579560525715351\n",
      "Iteration 7000, Loss: 0.014586646109819412\n",
      "Iteration 8000, Loss: 0.013588636182248592\n",
      "Iteration 9000, Loss: 0.012743335217237473\n",
      "Evaluating on the training data\n",
      "Accuracy: 0.9961857795715332\n",
      "Sensitivity: 0.9948892593383789\n",
      "Specificity: 0.9963824152946472\n",
      "Precision: 0.9765886068344116\n",
      "Evaluating on the validation data\n",
      "Accuracy: 0.9901345372200012\n",
      "Sensitivity: 0.981249988079071\n",
      "Specificity: 0.991623044013977\n",
      "Precision: 0.9515151381492615\n",
      "Transforming custom messages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 39.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9989],\n",
      "        [0.9997],\n",
      "        [0.0357]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = (df[\"type\"] == \"spam\").astype(int)\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "# Split the data\n",
    "df_train, df_val = train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "# Load BART and tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model_bart = BartModel.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# Tokenize messages with a progress bar\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(texts.tolist(), \n",
    "                     padding=True, \n",
    "                     truncation=True, \n",
    "                     return_tensors=\"pt\", \n",
    "                     max_length=512)\n",
    "\n",
    "# Transform training data with progress bar\n",
    "print(\"Tokenizing and transforming training data...\")\n",
    "tokens_train = tokenize_texts(df_train[\"message\"])\n",
    "embeddings_train = []\n",
    "for i in tqdm(range(len(tokens_train['input_ids']))):\n",
    "    with torch.no_grad():\n",
    "        output = model_bart(input_ids=tokens_train['input_ids'][i].unsqueeze(0), \n",
    "                            attention_mask=tokens_train['attention_mask'][i].unsqueeze(0))\n",
    "        embeddings_train.append(output.last_hidden_state.mean(dim=1).squeeze(0))\n",
    "\n",
    "# Transform validation data with progress bar\n",
    "print(\"Tokenizing and transforming validation data...\")\n",
    "tokens_val = tokenize_texts(df_val[\"message\"])\n",
    "embeddings_val = []\n",
    "for i in tqdm(range(len(tokens_val['input_ids']))):\n",
    "    with torch.no_grad():\n",
    "        output = model_bart(input_ids=tokens_val['input_ids'][i].unsqueeze(0), \n",
    "                            attention_mask=tokens_val['attention_mask'][i].unsqueeze(0))\n",
    "        embeddings_val.append(output.last_hidden_state.mean(dim=1).squeeze(0))\n",
    "\n",
    "# Stack the embeddings into tensors\n",
    "X_train = torch.stack(embeddings_train)\n",
    "y_train = torch.tensor(df_train[\"spam\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_val = torch.stack(embeddings_val)\n",
    "y_val = torch.tensor(df_val[\"spam\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Define a simple linear model\n",
    "model = nn.Linear(X_train.size(1), 1)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "\n",
    "# Training loop\n",
    "for i in range(10000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        print(f\"Iteration {i}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(X, y): \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = torch.sigmoid(model(X)) > 0.25\n",
    "        accuracy = (y_pred == y).type(torch.float32).mean().item()\n",
    "        sensitivity = (y_pred[y == 1] == y[y == 1]).type(torch.float32).mean().item()\n",
    "        specificity = (y_pred[y == 0] == y[y == 0]).type(torch.float32).mean().item()\n",
    "        precision = (y_pred[y_pred == 1] == y[y_pred == 1]).type(torch.float32).mean().item()\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Sensitivity: {sensitivity}\")\n",
    "        print(f\"Specificity: {specificity}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "\n",
    "# Evaluate on training data\n",
    "print(\"Evaluating on the training data\")\n",
    "evaluate_model(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation data\n",
    "print(\"Evaluating on the validation data\")\n",
    "evaluate_model(X_val, y_val)\n",
    "\n",
    "# Custom messages for prediction\n",
    "custom_messages = pd.Series([\n",
    "    \"We have released a new product, do you want to buy it?\", \n",
    "    \"Winner! Great deal, call us to get this product for free\",\n",
    "    \"Tomorrow is my birthday, do you want to come to the party?\"\n",
    "])\n",
    "\n",
    "tokens_custom = tokenize_texts(custom_messages)\n",
    "embeddings_custom = []\n",
    "print(\"Transforming custom messages...\")\n",
    "for i in tqdm(range(len(tokens_custom['input_ids']))):\n",
    "    with torch.no_grad():\n",
    "        output = model_bart(input_ids=tokens_custom['input_ids'][i].unsqueeze(0), \n",
    "                            attention_mask=tokens_custom['attention_mask'][i].unsqueeze(0))\n",
    "        embeddings_custom.append(output.last_hidden_state.mean(dim=1).squeeze(0))\n",
    "\n",
    "X_custom = torch.stack(embeddings_custom)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    pred = torch.sigmoid(model(X_custom))\n",
    "    print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5d2ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
