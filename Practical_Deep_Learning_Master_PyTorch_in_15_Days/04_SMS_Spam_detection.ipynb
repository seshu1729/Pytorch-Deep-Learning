{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be57a0e9",
   "metadata": {},
   "source": [
    "[Main code](#main-code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7957c24",
   "metadata": {},
   "source": [
    "First lets explore what is the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2b93cee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 12, saw 4\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data/SMSSpamCollection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m df.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 2 fields in line 12, saw 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8194dd3e",
   "metadata": {},
   "source": [
    "As it was not csv by default, we need to handle the data correctly.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "951aea72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ham  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4  spam   \n",
       "\n",
       "  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
       "0                      Ok lar... Joking wif u oni...                                                               \n",
       "1  Free entry in 2 a wkly comp to win FA Cup fina...                                                               \n",
       "2  U dun say so early hor... U c already then say...                                                               \n",
       "3  Nah I don't think he goes to usf, he lives aro...                                                               \n",
       "4  FreeMsg Hey there darling it's been 3 week's n...                                                               "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/SMSSpamCollection\", sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd84106",
   "metadata": {},
   "source": [
    "It was taking the 1st entry into the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c36ed6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/SMSSpamCollection\", sep=\"\\t\", names=[\"type\", \"message\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52a483d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type                                                     ham\n",
       "message    Go until jurong point, crazy.. Available only ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87bd766b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d3072b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2][\"message\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c4a14",
   "metadata": {},
   "source": [
    "How to know which message is spam. We can know it by checking type.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfbe3bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2        True\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "5567     True\n",
       "5568    False\n",
       "5569    False\n",
       "5570    False\n",
       "5571    False\n",
       "Name: type, Length: 5572, dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"type\"] == \"spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845d4157",
   "metadata": {},
   "source": [
    "To make the neuron learn easily, we can keep 0,1 in the column instead of ham and spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25852505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                            message   spam\n",
       "0      ham  Go until jurong point, crazy.. Available only ...  False\n",
       "1      ham                      Ok lar... Joking wif u oni...  False\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   True\n",
       "3      ham  U dun say so early hor... U c already then say...  False\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...  False\n",
       "...    ...                                                ...    ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...   True\n",
       "5568   ham               Will ü b going to esplanade fr home?  False\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...  False\n",
       "5570   ham  The guy did some bitching but I acted like i'd...  False\n",
       "5571   ham                         Rofl. Its true to its name  False\n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b178b608",
   "metadata": {},
   "source": [
    "So now we dont need type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03f7cecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                            message   spam\n",
       "0   ham  Go until jurong point, crazy.. Available only ...  False\n",
       "1   ham                      Ok lar... Joking wif u oni...  False\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   True\n",
       "3   ham  U dun say so early hor... U c already then say...  False\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...  False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9eefec",
   "metadata": {},
   "source": [
    "It didn't drop correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4efee6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message   spam\n",
       "0  Go until jurong point, crazy.. Available only ...  False\n",
       "1                      Ok lar... Joking wif u oni...  False\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   True\n",
       "3  U dun say so early hor... U c already then say...  False\n",
       "4  Nah I don't think he goes to usf, he lives aro...  False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95175615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of SPAM messages:\n",
      "747\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of SPAM messages:\")\n",
    "print(len(df[df[\"spam\"]==True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d0894c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Not SPAM messages:\n",
      "4825\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Not SPAM messages:\")\n",
    "print(len(df[df[\"spam\"]==False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd510b",
   "metadata": {},
   "source": [
    "Lets learn how CountVectorizer works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98032178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing' 'hello' 'is' 'mars' 'perfect' 'today']\n",
      "=======\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 9 stored elements and shape (2, 6)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t3\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "=======\n",
      "[[1 3 1 0 0 1]\n",
      " [0 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=6)\n",
    "documents = [\n",
    "    \"Hello world. Today is amazing. Hello hello\",\n",
    "    \"Hello mars, today is perfect\"\n",
    "]\n",
    "cv.fit(documents)\n",
    "print(cv.get_feature_names_out())\n",
    "out = cv.transform(documents)\n",
    "print(\"=======\")\n",
    "print(out)\n",
    "print(\"=======\")\n",
    "print(out.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd47f61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing' 'hello' 'is' 'mars' 'perfect' 'today' 'world']\n",
      "=======\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 10 stored elements and shape (2, 7)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t3\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "=======\n",
      "[[1 3 1 0 0 1 1]\n",
      " [0 1 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_features=7) # Change this from 1 to 7.  even if you increase beyond 7, it will not change because there are only 7 unique words here\n",
    "documents = [\n",
    "    \"Hello world. Today is amazing. Hello hello\",\n",
    "    \"Hello mars, today is perfect\"\n",
    "]\n",
    "cv.fit(documents)\n",
    "print(cv.get_feature_names_out())\n",
    "out = cv.transform(documents)\n",
    "print(\"=======\")\n",
    "print(out)\n",
    "print(\"=======\")\n",
    "print(out.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536d99d9",
   "metadata": {},
   "source": [
    "Lets apply this CountVectorizer to our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8159293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 12 stored elements and shape (1, 1000)>\n",
      "  Coords\tValues\n",
      "  (0, 349)\t1\n",
      "  (0, 887)\t1\n",
      "  (0, 661)\t1\n",
      "  (0, 206)\t1\n",
      "  (0, 90)\t1\n",
      "  (0, 613)\t1\n",
      "  (0, 429)\t1\n",
      "  (0, 361)\t1\n",
      "  (0, 973)\t1\n",
      "  (0, 830)\t1\n",
      "  (0, 358)\t1\n",
      "  (0, 921)\t1\n",
      "-------\n",
      "up\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_features=1000)\n",
    "messages = cv.fit_transform(df[\"message\"])\n",
    "print(messages[0, :])\n",
    "print(\"-------\")\n",
    "print(cv.get_feature_names_out()[888])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0ab70",
   "metadata": {},
   "source": [
    "There was an alternative for CountVectorizer. i.e, TFidfVectorizer Which gives slightly better results. \n",
    "Just use this Tfidf inplace of cv. no big change here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa3dfed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 12 stored elements and shape (1, 1000)>\n",
      "  Coords\tValues\n",
      "  (0, 349)\t0.21767405036534812\n",
      "  (0, 887)\t0.3385917224021876\n",
      "  (0, 661)\t0.37588329053830144\n",
      "  (0, 206)\t0.3721714491861035\n",
      "  (0, 90)\t0.3594536477277268\n",
      "  (0, 613)\t0.22967925385076954\n",
      "  (0, 429)\t0.15749629055186737\n",
      "  (0, 361)\t0.26546931912738453\n",
      "  (0, 973)\t0.3250709155826965\n",
      "  (0, 830)\t0.2289232644503655\n",
      "  (0, 358)\t0.22529489923658547\n",
      "  (0, 921)\t0.26851543603096684\n",
      "up\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "messages = vectorizer.fit_transform(df[\"message\"])\n",
    "print(messages[0, :])\n",
    "print(vectorizer.get_feature_names_out()[888])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc037c5a",
   "metadata": {},
   "source": [
    "Now lets train the neuron. Lets start with what variables we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a15d838",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse array length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m cv = CountVectorizer(max_features=\u001b[32m1000\u001b[39m)\n\u001b[32m     14\u001b[39m messages = cv.fit_transform(df[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m X = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(X)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# y = torch.tensor(df[\"spam\"], dtype=torch.float32)\\\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#         .reshape((-1, 1))\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\scipy\\sparse\\_base.py:449\u001b[39m, in \u001b[36m_spbase.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msparse array length is ambiguous; use getnnz()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33m or shape[0]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: sparse array length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "cv = CountVectorizer(max_features=1000)\n",
    "messages = cv.fit_transform(df[\"message\"])\n",
    "\n",
    "X = torch.tensor(messages, dtype=torch.float32)\n",
    "print(X)\n",
    "# y = torch.tensor(df[\"spam\"], dtype=torch.float32)\\\n",
    "#         .reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c408c78",
   "metadata": {},
   "source": [
    "X was giving some errors, the error says it is a sparse matrix while converting it into tensor. We need to convert it into dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "409e5df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([5572, 1000])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(messages.todense(), dtype=torch.float32)\n",
    "# y = torch.tensor(df[\"spam\"], dtype=torch.float32)\\\n",
    "#         .reshape((-1, 1))\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c2d8b",
   "metadata": {},
   "source": [
    "lets see y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e4ce1c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1.,  ..., 0., 0., 0.])\n",
      "torch.Size([5572])\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor(df[\"spam\"], dtype=torch.float32)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b367395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        ...,\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "torch.Size([5572, 1])\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor(df[\"spam\"], dtype=torch.float32).reshape((-1,1))\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "be628cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1290, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1125, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1027, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0861, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0821, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0676, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0654, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0634, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0616, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0583, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0569, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0542, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0531, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0510, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0500, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0491, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0483, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0475, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0467, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0460, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0453, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0447, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0441, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0435, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0430, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0425, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0420, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0406, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0402, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0398, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0394, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0391, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0387, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0384, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0381, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0378, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0375, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0372, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0369, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0366, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0364, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0361, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0359, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0356, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0354, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0352, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0350, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0348, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0345, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0343, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0342, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0338, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0336, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0334, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0333, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0331, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0329, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0328, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0326, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0325, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0323, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0322, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0320, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0319, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0318, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0316, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0315, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0314, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0312, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0311, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0310, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0309, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0308, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0306, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0305, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0304, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0303, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0302, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0301, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0300, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0299, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0298, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0297, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0296, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0295, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0294, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0293, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0292, grad_fn=<MseLossBackward0>)\n",
      "tensor([[-0.0028],\n",
      "        [ 0.0315],\n",
      "        [ 0.8789],\n",
      "        ...,\n",
      "        [-0.0185],\n",
      "        [ 0.1793],\n",
      "        [ 0.0741]])\n",
      "tensor(-0.6144)\n",
      "tensor(1.5018)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "cv = CountVectorizer(max_features=1000)\n",
    "messages = cv.fit_transform(df[\"message\"])\n",
    "\n",
    "X = torch.tensor(messages.todense(), dtype=torch.float32)\n",
    "y = torch.tensor(df[\"spam\"], dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "model = nn.Linear(1000, 1)  # our X has 1 thousand entries as input and we need to get only 1 output\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = loss_fn(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0: \n",
    "        print(loss)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "    print(y_pred)\n",
    "    print(y_pred.min())\n",
    "    print(y_pred.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0d1ee9",
   "metadata": {},
   "source": [
    "In this use case we should not get negative values of probability. So we need to use a Sigmoid function. Which only gives 0 or 1 as output. Sigmoid example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2185ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "93ec7094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.8808, 0.9526])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.sigmoid(torch.tensor([-100, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "015d9905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21167777c50>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = torch.arange(-5, 5, step=0.1)\n",
    "y = nn.functional.sigmoid(X)\n",
    "plt.plot(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685fd2bf",
   "metadata": {},
   "source": [
    "We are changing only the loss function not the neuron. Keep in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3211adaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6846, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5383, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4686, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4268, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.3974, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.3745, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.3555, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.3391, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.3246, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.3117, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.3001, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2895, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2799, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2711, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2630, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2555, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2486, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2422, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2363, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2308, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2256, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2207, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2162, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2119, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2079, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2041, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2005, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1971, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1938, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1907, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1878, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1850, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1823, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1798, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1774, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1750, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1728, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1706, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1686, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1666, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1647, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1628, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1610, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1593, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1577, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1561, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1545, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1530, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1516, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1502, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1488, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1475, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1462, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1450, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1438, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1426, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1415, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1403, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1393, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1382, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1372, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1362, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1352, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1343, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1333, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1324, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1315, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1307, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1298, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1290, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1282, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1274, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1266, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1259, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1252, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1244, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1237, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1230, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1223, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1217, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1210, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1204, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1198, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1191, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1185, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1179, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1174, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1168, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1162, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1157, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1151, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1146, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1141, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1135, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1130, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1125, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1120, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1116, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1111, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1106, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor([[-3.6595],\n",
      "        [-3.3020],\n",
      "        [ 2.6768],\n",
      "        ...,\n",
      "        [-3.6798],\n",
      "        [-2.8184],\n",
      "        [-2.5432]])\n",
      "tensor(-16.6541)\n",
      "tensor(7.3566)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "cv = CountVectorizer(max_features=1000)\n",
    "messages = cv.fit_transform(df[\"message\"])\n",
    "\n",
    "X = torch.tensor(messages.todense(), dtype=torch.float32)\n",
    "y = torch.tensor(df[\"spam\"], dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "model = nn.Linear(1000, 1)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = loss_fn(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(loss)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "    print(y_pred)\n",
    "    print(y_pred.min())\n",
    "    print(y_pred.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db34ec4",
   "metadata": {},
   "source": [
    "Still the output contains the negative values. increasing learning rate from 0.01 to 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "634ea06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6947, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2245, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1639, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1366, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1205, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1097, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1018, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0957, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0908, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0867, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor([[0.0126],\n",
      "        [0.0213],\n",
      "        [0.9715],\n",
      "        ...,\n",
      "        [0.0144],\n",
      "        [0.0427],\n",
      "        [0.0384]])\n",
      "tensor(1.3779e-10)\n",
      "tensor(0.9999)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(1000, 1)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = loss_fn(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        print(loss)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = nn.functional.sigmoid(model(X)) # model will be predicted and sent to sigmoid function to get the values from 0 to 1 only\n",
    "    print(y_pred)\n",
    "    print(y_pred.min())\n",
    "    print(y_pred.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c781b133",
   "metadata": {},
   "source": [
    "Now how to evaluate the model with performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6f70e7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7042, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2250, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1641, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1367, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1207, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1098, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1019, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0958, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0909, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0868, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "accuracy: tensor(0.9803)\n",
      "sensitivity: tensor(0.9224)\n",
      "specificity: tensor(0.9892)\n",
      "precision: tensor(0.9298)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "cv = CountVectorizer(max_features=1000)\n",
    "messages = cv.fit_transform(df[\"message\"])\n",
    "\n",
    "X = torch.tensor(messages.todense(), dtype=torch.float32)\n",
    "y = torch.tensor(df[\"spam\"], dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "model = nn.Linear(1000, 1)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = loss_fn(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        print(loss)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = nn.functional.sigmoid(model(X)) > 0.25\n",
    "    print(\"accuracy:\", (y_pred == y)\\\n",
    "        .type(torch.float32).mean())\n",
    "    \n",
    "    print(\"sensitivity:\", (y_pred[y == 1] == y[y == 1])\\\n",
    "        .type(torch.float32).mean())\n",
    "    \n",
    "    print(\"specificity:\", (y_pred[y == 0] == y[y == 0])\\\n",
    "        .type(torch.float32).mean())\n",
    "\n",
    "    print(\"precision:\", (y_pred[y_pred == 1] == y[y_pred == 1])\\\n",
    "        .type(torch.float32).mean()) \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ba473a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1341)\n",
      "tensor(0.8659)\n",
      "======\n",
      "tensor(0.1330)\n",
      "tensor(0.8670)\n"
     ]
    }
   ],
   "source": [
    "print((y == 1).type(torch.float32).mean())\n",
    "print((y == 0).type(torch.float32).mean())\n",
    "print(\"======\")\n",
    "print((y_pred == 1).type(torch.float32).mean())\n",
    "print((y_pred == 0).type(torch.float32).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "76d271d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(0.)\n",
      "======\n",
      "tensor(0.9298)\n",
      "tensor(0.0120)\n"
     ]
    }
   ],
   "source": [
    "print((y[y == 1]).type(torch.float32).mean())\n",
    "print((y[y == 0]).type(torch.float32).mean())\n",
    "print(\"======\")\n",
    "print((y[y_pred == 1]).type(torch.float32).mean())\n",
    "print((y[y_pred == 0]).type(torch.float32).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7edf1e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9224)\n",
      "tensor(0.0108)\n",
      "======\n",
      "tensor(1.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print((y_pred[y == 1]).type(torch.float32).mean())\n",
    "print((y_pred[y == 0]).type(torch.float32).mean())\n",
    "print(\"======\")\n",
    "print((y_pred[y_pred == 1]).type(torch.float32).mean())\n",
    "print((y_pred[y_pred == 0]).type(torch.float32).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25214f5",
   "metadata": {},
   "source": [
    "Let's train the model with training data and do the validation with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "34d7e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "df_train = df.sample(frac=0.8, random_state=0)\n",
    "df_val = df.drop(index=df_train.index)\n",
    "\n",
    "cv = CountVectorizer(max_features=5000)\n",
    "messages_train = cv.fit_transform(df_train[\"message\"])\n",
    "messages_val = cv.transform(df_val[\"message\"])\n",
    "\n",
    "X_train = torch.tensor(messages_train.todense(), dtype=torch.float32)\n",
    "y_train = torch.tensor(df_train[\"spam\"].values, dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "X_val = torch.tensor(messages_val.todense(), dtype=torch.float32)\n",
    "y_val = torch.tensor(df_val[\"spam\"].values, dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "model = nn.Linear(5000, 1)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "79759246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X, y): \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = nn.functional.sigmoid(model(X)) > 0.25\n",
    "        print(\"accuracy:\", (y_pred == y)\\\n",
    "            .type(torch.float32).mean())\n",
    "        \n",
    "        print(\"sensitivity:\", (y_pred[y == 1] == y[y == 1])\\\n",
    "            .type(torch.float32).mean())\n",
    "        \n",
    "        print(\"specificity:\", (y_pred[y == 0] == y[y == 0])\\\n",
    "            .type(torch.float32).mean())\n",
    "\n",
    "        print(\"precision:\", (y_pred[y_pred == 1] == y[y_pred == 1])\\\n",
    "            .type(torch.float32).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "14ee3bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6917, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Evaluating on the training data\n",
      "accuracy: tensor(0.9632)\n",
      "sensitivity: tensor(0.8569)\n",
      "specificity: tensor(0.9800)\n",
      "precision: tensor(0.8712)\n",
      "Evaluating on the validation data\n",
      "accuracy: tensor(0.9632)\n",
      "sensitivity: tensor(0.8633)\n",
      "specificity: tensor(0.9774)\n",
      "precision: tensor(0.8451)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 1000): # Change from 1000, 5000, to 10000 and see the differen\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        print(loss) \n",
    "        \n",
    "print(\"Evaluating on the training data\")\n",
    "evaluate_model(X_train, y_train)\n",
    "\n",
    "print(\"Evaluating on the validation data\")\n",
    "evaluate_model(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8281f7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2218, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1604, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1325, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1160, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1047, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Evaluating on the training data\n",
      "accuracy: tensor(0.9776)\n",
      "sensitivity: tensor(0.9178)\n",
      "specificity: tensor(0.9870)\n",
      "precision: tensor(0.9178)\n",
      "Evaluating on the validation data\n",
      "accuracy: tensor(0.9740)\n",
      "sensitivity: tensor(0.9137)\n",
      "specificity: tensor(0.9826)\n",
      "precision: tensor(0.8819)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5000): # Change from 1000, 5000, to 10000 and see the differen\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        print(loss) \n",
    "        \n",
    "print(\"Evaluating on the training data\")\n",
    "evaluate_model(X_train, y_train)\n",
    "\n",
    "print(\"Evaluating on the validation data\")\n",
    "evaluate_model(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c49e443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0964, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0900, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0847, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0804, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0766, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0734, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0705, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0680, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0657, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0636, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Evaluating on the training data\n",
      "accuracy: tensor(0.9841)\n",
      "sensitivity: tensor(0.9391)\n",
      "specificity: tensor(0.9912)\n",
      "precision: tensor(0.9438)\n",
      "Evaluating on the validation data\n",
      "accuracy: tensor(0.9794)\n",
      "sensitivity: tensor(0.9209)\n",
      "specificity: tensor(0.9877)\n",
      "precision: tensor(0.9143)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10000): # Change from 1000, 5000, to 10000 and see the differen\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)\n",
    "\n",
    "print(\"Evaluating on the training data\")\n",
    "evaluate_model(X_train, y_train)\n",
    "\n",
    "print(\"Evaluating on the validation data\")\n",
    "evaluate_model(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a0a60",
   "metadata": {},
   "source": [
    "If you observe the above one, it has a good precision than others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8386dce",
   "metadata": {},
   "source": [
    "Now the model is ready for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf70dae3",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4793ba3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6972, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.2250, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1642, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1366, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1203, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1093, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.1013, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0950, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0900, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0858, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Evaluating on the training data\n",
      "accuracy: tensor(0.9787)\n",
      "sensitivity: tensor(0.9211)\n",
      "specificity: tensor(0.9878)\n",
      "precision: tensor(0.9226)\n",
      "Evaluating on the validation data\n",
      "accuracy: tensor(0.9740)\n",
      "sensitivity: tensor(0.9137)\n",
      "specificity: tensor(0.9826)\n",
      "precision: tensor(0.8819)\n",
      "tensor([[0.0524],\n",
      "        [0.6096],\n",
      "        [0.0129]])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "df_train = df.sample(frac=0.8, random_state=0)\n",
    "df_val = df.drop(index=df_train.index)\n",
    "\n",
    "cv = CountVectorizer(max_features=1000)\n",
    "messages_train = cv.fit_transform(df_train[\"message\"])\n",
    "messages_val = cv.transform(df_val[\"message\"])\n",
    "\n",
    "X_train = torch.tensor(messages_train.todense(), dtype=torch.float32)\n",
    "y_train = torch.tensor(df_train[\"spam\"].values, dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "X_val = torch.tensor(messages_val.todense(), dtype=torch.float32)\n",
    "y_val = torch.tensor(df_val[\"spam\"].values, dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "model = nn.Linear(1000, 1)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        print(loss)\n",
    "\n",
    "def evaluate_model(X, y): \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = nn.functional.sigmoid(model(X)) > 0.25\n",
    "        print(\"accuracy:\", (y_pred == y)\\\n",
    "            .type(torch.float32).mean())\n",
    "        \n",
    "        print(\"sensitivity:\", (y_pred[y == 1] == y[y == 1])\\\n",
    "            .type(torch.float32).mean())\n",
    "        \n",
    "        print(\"specificity:\", (y_pred[y == 0] == y[y == 0])\\\n",
    "            .type(torch.float32).mean())\n",
    "\n",
    "        print(\"precision:\", (y_pred[y_pred == 1] == y[y_pred == 1])\\\n",
    "            .type(torch.float32).mean()) \n",
    "        \n",
    "print(\"Evaluating on the training data\")\n",
    "evaluate_model(X_train, y_train)\n",
    "\n",
    "print(\"Evaluating on the validation data\")\n",
    "evaluate_model(X_val, y_val)\n",
    "\n",
    "custom_messages = cv.transform([\n",
    "    \"We have release a new product, do you want to buy it?\", \n",
    "    \"Winner! Great deal, call us to get this product for free\",\n",
    "    \"Tomorrow is my birthday, do you come to the party?\"\n",
    "])\n",
    "\n",
    "X_custom = torch.tensor(custom_messages.todense(), dtype=torch.float32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = nn.functional.sigmoid(model(X_custom))\n",
    "    print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b120058",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47426f95",
   "metadata": {},
   "source": [
    "### Another way using transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2626d3f",
   "metadata": {},
   "source": [
    "Normal way to generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1319195d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 170, 33, 800, 10, 92, 1152, 6, 109, 47, 236, 7, 907, 24, 116, 2], [0, 46722, 328, 2860, 432, 6, 486, 201, 7, 120, 42, 1152, 13, 481, 2], [0, 38849, 16, 127, 4115, 6, 109, 47, 283, 7, 5, 537, 116, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "messages = [\n",
    "    \"We have release a new product, do you want to buy it?\", \n",
    "    \"Winner! Great deal, call us to get this product for free\",\n",
    "    \"Tomorrow is my birthday, do you come to the party?\",\n",
    "]\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "out = tokenizer(messages)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e89ea943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 170, 33, 800, 10, 92, 1152, 6, 109, 47, 236, 7, 907, 24, 116, 2], [0, 46722, 328, 2860, 432, 6, 486, 201, 7, 120, 42, 1152, 13, 481, 2, 1], [0, 38849, 16, 127, 4115, 6, 109, 47, 283, 7, 5, 537, 116, 2, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# Observe the padding difference\n",
    "out = tokenizer(messages, padding=True)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6a47b8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 170, 33, 800, 10, 92, 1152, 6, 109, 47, 236, 7, 907, 24, 116, 2], [0, 46722, 328, 2860, 432, 6, 486, 201, 7, 120, 42, 1152, 13, 481, 2, 1], [0, 38849, 16, 127, 4115, 6, 109, 47, 283, 7, 5, 537, 116, 2, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# Now it says to process only 512 tokens to process. Only 512 tokens not 512 characters. If it exceed 512 tokens do the truncation\n",
    "out = tokenizer(messages, \n",
    "                padding=True, \n",
    "                max_length=512, \n",
    "                truncation=True) # \n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "941a9fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   170,    33,   800,    10,    92,  1152,     6,   109,    47,\n",
      "           236,     7,   907,    24,   116,     2],\n",
      "        [    0, 46722,   328,  2860,   432,     6,   486,   201,     7,   120,\n",
      "            42,  1152,    13,   481,     2,     1],\n",
      "        [    0, 38849,    16,   127,  4115,     6,   109,    47,   283,     7,\n",
      "             5,   537,   116,     2,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n",
      "torch.Size([3, 16, 768])\n",
      "torch.Size([3, 768])\n",
      "tensor([-4.1475e-01, -9.5950e-01, -1.9372e+00, -1.8202e+00, -1.3529e+00,\n",
      "        -3.2617e-01, -4.3142e-02, -2.8496e-01, -4.4574e-01, -2.8201e+00,\n",
      "        -7.7632e-01,  9.4290e-01,  3.7461e-01,  3.3576e-01,  5.2033e-01,\n",
      "         7.1250e-01, -9.4961e-01,  7.0305e-01, -3.9577e-01, -1.0670e+00,\n",
      "         2.0777e-01, -1.6843e-01, -1.0866e+00,  8.7861e-01,  3.0825e-01,\n",
      "        -8.2243e-01,  5.6214e-01,  2.9970e+00,  3.1303e-01, -3.6227e+00,\n",
      "         6.2353e-01,  3.4955e-01,  1.5166e-01, -9.7267e-01, -1.5525e+00,\n",
      "         1.6685e+00, -1.1180e-01,  1.4668e+00,  1.0774e-01, -2.3462e-01,\n",
      "         3.8726e-01,  1.2186e+00,  4.9061e-01, -4.8987e-01,  3.9203e-01,\n",
      "         4.2587e-01, -1.9299e+00,  5.9904e-01, -6.1466e-01,  7.6263e-01,\n",
      "        -1.3223e+00,  6.8178e-01, -8.5019e-01,  1.1885e-01, -6.4347e-02,\n",
      "        -1.0802e+00,  3.9521e-01, -1.3715e-02, -1.4325e-01,  3.8025e-01,\n",
      "         2.1685e-01,  7.3130e-01,  3.3559e-01,  6.3142e-01, -7.0818e-01,\n",
      "        -1.4830e-01,  7.6586e-01,  5.2708e-01,  9.9074e-01,  1.5059e-01,\n",
      "         3.7120e-01, -2.2960e+00, -5.4099e-01,  6.0844e-02, -6.0287e-01,\n",
      "         4.0135e-01, -1.3555e+00, -8.2851e-01,  5.8254e-02, -5.3966e-01,\n",
      "         3.3954e-01, -4.1404e-01,  7.1217e-01,  7.1192e-02,  3.3282e-01,\n",
      "        -8.0114e-01, -1.8887e-01,  1.0675e-01,  6.4967e-01,  7.1149e-02,\n",
      "        -5.4697e-01,  2.0469e-01,  1.0316e+00, -2.8317e-01,  7.7322e-01,\n",
      "         1.8169e+00,  1.3140e-01, -1.0399e+00, -1.5442e-01, -1.8674e-01,\n",
      "         1.1935e+00,  8.4956e-01, -1.1487e+00,  1.8479e-01,  1.7713e+00,\n",
      "         2.9089e-01,  1.0326e+00,  5.9302e-01,  4.7164e-01,  2.2687e-01,\n",
      "        -1.4431e+00,  2.6868e-01,  6.6778e-01, -1.5047e-01, -7.7921e-01,\n",
      "        -4.5424e-01, -1.4516e+00, -7.8665e-01,  2.0442e+00, -4.3260e-01,\n",
      "         2.0610e+00,  7.0859e-01, -7.7400e-01,  7.4771e-01,  1.1789e-01,\n",
      "        -2.6061e-02, -1.0395e+00,  7.3904e-01, -1.1985e+00, -1.8468e-01,\n",
      "         8.6259e-01,  1.5406e+00,  1.3009e+00, -9.2566e-02,  2.5857e-01,\n",
      "        -1.3698e+00,  4.3143e-01,  9.4272e-01,  9.5243e-01, -3.1455e-01,\n",
      "         1.1327e-01, -8.5425e-01, -2.0648e-01, -9.7961e-01,  1.7658e-01,\n",
      "         2.2374e-01, -9.8801e-02, -1.4849e-01,  3.2218e-01,  7.8808e-01,\n",
      "         2.5684e-01,  7.6109e-01,  6.6792e-01,  4.9622e-01, -4.2776e-02,\n",
      "         7.5357e-01, -3.7806e-01,  9.1128e-01,  8.9477e-01, -2.5515e-01,\n",
      "        -6.1028e-01,  1.0094e-01, -4.0385e-01,  5.1562e-02, -2.6430e-01,\n",
      "        -5.6223e-01,  4.2568e-01,  6.4352e-01,  8.1615e-01,  2.2419e-01,\n",
      "        -6.2836e-02,  7.7046e-01, -1.1277e+00,  9.8505e-01, -2.1810e-01,\n",
      "        -5.7274e-01, -6.7072e-01, -5.3110e-01,  1.3541e-01,  6.0107e-01,\n",
      "         4.3962e-01,  1.8937e-01,  4.5798e-01, -1.1746e+00,  4.9404e-01,\n",
      "         7.9294e-01,  7.9963e-01,  5.7125e-01, -7.7112e-01, -1.0891e+00,\n",
      "         6.6052e-01,  3.8090e-01,  1.8508e-01, -8.2274e-01, -1.5393e-01,\n",
      "         3.9580e-01, -3.0838e-01,  1.3217e+00,  1.0522e+00,  1.8827e-01,\n",
      "         1.3554e+00, -1.1751e-01,  6.2035e-01,  5.5142e-01,  1.9135e+00,\n",
      "        -5.8111e-01,  6.9197e-01, -2.2210e-01, -3.9427e-01,  4.9954e-01,\n",
      "        -4.7491e-01, -5.1088e-02,  9.0249e-01,  1.0499e+00, -7.2501e-01,\n",
      "         6.9397e-02, -1.3099e-01, -8.0542e-01,  1.7105e+00, -1.4883e+00,\n",
      "         3.4443e-01, -1.1071e+00,  1.1600e+00,  2.1013e-01,  1.1625e+00,\n",
      "         3.8705e-01, -5.8038e-01,  1.7525e+00,  3.4043e-01,  4.5692e-01,\n",
      "         5.6630e-01, -2.2989e-02, -1.4226e-01, -3.5858e-01, -2.0539e-01,\n",
      "         1.0087e+00, -1.2032e+00,  1.1797e+00, -1.4366e+00, -5.3436e-01,\n",
      "         1.1397e-01,  2.2150e+00,  9.5714e-01,  1.6787e+00, -7.7719e-01,\n",
      "         8.8115e-01,  2.6390e-01, -3.4971e-01,  9.6814e-01,  7.4955e-01,\n",
      "        -5.3866e-01,  5.1850e-01, -1.3745e+00, -5.3499e-01,  2.8552e-01,\n",
      "        -6.2070e-01,  3.9283e-01,  7.1961e-01, -6.6272e-02,  6.3013e-01,\n",
      "        -7.6092e-01,  1.2916e-01, -5.2718e-01, -4.8940e-01,  5.4991e-01,\n",
      "         3.2553e-01, -2.4546e-01,  1.0580e+00,  2.0011e-02, -8.6345e-01,\n",
      "        -9.9340e-01,  5.9760e-02,  2.5060e-01, -4.0710e-01,  7.3701e-01,\n",
      "        -3.7636e-01,  9.4704e-02, -2.5705e-01, -4.8306e-01,  1.2695e+00,\n",
      "         1.3508e+00,  1.2664e+00,  9.7701e-01, -2.0097e+00,  2.3033e-01,\n",
      "        -2.6531e-01,  4.9801e-01,  5.6048e-01,  4.4079e-02, -2.2131e-01,\n",
      "         1.6945e+00,  1.3591e+00, -1.1824e+00, -3.9429e-01,  1.4459e+00,\n",
      "        -5.0818e-01, -1.2323e+00,  1.8119e-01, -6.0612e-01,  1.4180e-01,\n",
      "        -1.2813e+00,  8.3170e-01, -8.5895e-03,  1.0103e+00, -6.4637e-02,\n",
      "         2.0448e-01, -5.9049e-02, -8.4107e-02, -4.2917e-01, -8.7000e-01,\n",
      "         8.8375e-01,  3.1787e-01, -4.0937e-01, -5.6909e-02,  1.3654e+00,\n",
      "         1.1932e+00,  3.2359e-01, -4.6136e-01, -2.2596e-01,  7.1680e-02,\n",
      "         9.9944e-01,  5.0000e-01, -3.4226e-01, -1.3771e-01, -3.8507e-01,\n",
      "        -2.3808e-02,  4.7299e-01, -1.5213e-01,  7.3155e-01,  1.5441e+00,\n",
      "        -3.1469e-01,  5.9238e-01,  4.2674e+00,  4.1413e-01,  1.6818e-01,\n",
      "        -1.2494e+00,  2.7738e-01,  7.4282e-02,  2.3215e-01,  3.6279e-02,\n",
      "         7.7495e-01, -2.1801e-01, -8.6731e-01,  6.0887e-01,  1.3523e+00,\n",
      "         8.0288e-01,  7.0077e-01,  1.7358e-01, -7.6069e-01, -1.6251e-01,\n",
      "        -7.6872e-01, -1.2867e+00,  8.9310e-03, -4.8212e-01,  1.5390e+00,\n",
      "        -5.8121e-01,  3.0819e-01, -5.9069e-01,  1.6313e+00,  8.9079e-01,\n",
      "         1.0093e-01, -8.9053e-01,  6.9939e-02,  2.0636e-01, -7.1217e-01,\n",
      "         2.7811e-03, -1.2153e-01,  7.6926e-01, -5.6563e-01, -2.2855e-01,\n",
      "        -1.1961e+00,  4.1718e-01,  7.0554e-01, -1.5196e+00, -1.6051e+00,\n",
      "        -1.6529e-01,  9.7834e-01, -1.5735e+00,  6.2087e-01,  6.3398e-01,\n",
      "        -6.1600e-01,  1.2622e-01,  8.4936e-02,  1.9111e-01,  1.7907e-01,\n",
      "         1.9763e+00,  1.0605e+00, -9.5546e-01, -2.1608e+00,  7.2820e-01,\n",
      "         9.9788e-01,  1.5679e-01,  1.1343e+00,  1.2692e-01,  8.6366e-01,\n",
      "         5.6870e-01, -1.1499e-02,  1.4351e+00,  4.6019e-01,  9.9197e-01,\n",
      "         1.5703e+00, -1.6681e-01,  1.3047e+00,  1.6867e-01, -1.2434e+00,\n",
      "         2.9224e+00, -2.7053e-02,  5.7483e-01,  6.5371e-01,  3.1998e-01,\n",
      "        -5.6605e-01,  2.2950e-01,  3.5763e-01,  1.4066e-01,  1.0723e+00,\n",
      "         1.4006e-01, -3.0236e-01,  2.3099e-01, -1.4623e+00, -7.5033e-01,\n",
      "         2.0951e-01, -2.6283e-01, -3.1380e-01, -1.7540e+00,  4.8429e-01,\n",
      "        -2.7652e-01,  5.0351e-01,  3.2027e-01, -2.9000e-01,  3.3029e-01,\n",
      "         3.2933e-01,  2.9229e-02,  3.7104e-01,  6.2425e-01,  2.7658e-01,\n",
      "         3.2014e-01,  5.0463e-01, -1.6570e+00, -4.1505e-01,  8.7000e-01,\n",
      "        -1.3688e+00, -2.6004e-01,  8.1837e-01, -6.3115e-01, -2.5408e-01,\n",
      "        -6.1117e-01,  1.8765e-01,  5.9824e-01,  4.5026e-01,  1.0128e+00,\n",
      "        -1.7212e-01,  1.9344e-01,  3.0133e-01,  6.9407e-01,  3.4426e-01,\n",
      "         7.7636e-01, -6.8282e-01, -2.7234e-01,  2.6596e-01, -1.3303e+00,\n",
      "        -7.2119e-01,  1.6488e-01,  1.8021e-02,  3.7510e-01, -4.0045e-01,\n",
      "        -9.2344e-02,  1.8157e-02,  1.3516e-01,  7.6951e-01,  3.3846e-01,\n",
      "        -1.2066e-01, -3.6541e-01, -4.0758e-01, -2.2845e-03, -5.8193e-01,\n",
      "         6.5466e-01,  5.0772e-01, -2.9894e-01,  2.2297e-01, -6.9664e-01,\n",
      "         4.4005e-01,  7.0804e-01, -2.1537e-01,  1.0084e+00, -7.6860e-01,\n",
      "         9.7583e-01, -3.3504e-01, -6.4838e-01,  1.7211e+00,  1.0415e+00,\n",
      "         5.8568e-01,  2.9294e-01, -2.9926e-01, -9.1095e-02,  2.0280e-01,\n",
      "         6.9967e-01, -1.1933e-01, -9.6195e+00,  3.7097e-01,  9.4654e-01,\n",
      "         3.2910e-01, -4.5670e-02,  2.1232e-01, -3.5392e-01,  1.0385e+00,\n",
      "         4.4189e-01,  1.4281e+00,  5.5003e-01,  9.7709e-01,  8.6210e-01,\n",
      "         1.1561e-01,  1.4830e+00,  1.7212e+00,  4.9961e-03, -2.7197e-01,\n",
      "        -4.3734e-01,  1.1509e-01, -1.2504e-01, -6.5717e-01,  9.6498e-01,\n",
      "        -1.0891e-01, -1.1341e+00, -3.5492e-01, -1.0765e+00, -1.3831e-01,\n",
      "        -1.1789e-01,  1.2428e+00, -3.5665e-01,  3.3932e-01, -3.3820e-01,\n",
      "        -5.0657e-02, -1.0472e+00, -6.0159e-02, -1.2930e-01,  7.4063e-01,\n",
      "        -1.1751e-01, -2.0110e-01,  2.5657e-01, -3.1756e-01,  1.0400e+00,\n",
      "        -4.7883e-01,  6.1821e-03,  9.6114e-01, -7.0611e-01, -2.3108e-01,\n",
      "        -3.2789e-02, -7.3197e-02, -3.2422e-01,  6.7265e-01,  4.5614e-03,\n",
      "         1.6397e-01,  1.1572e-01, -3.4807e-01,  3.5522e-02, -7.6326e-01,\n",
      "        -9.8880e-01,  4.2391e-01, -1.0604e+00,  4.4391e-01,  7.3092e-01,\n",
      "        -2.7843e-01,  7.7305e-01,  1.0912e-02,  1.2804e-01, -1.9877e+00,\n",
      "        -4.1987e-01, -6.7400e-02,  1.6773e+00, -2.8985e-01,  8.0948e-02,\n",
      "         3.2138e-01,  1.2680e+00, -9.1868e-01, -4.4133e-01, -2.1895e+00,\n",
      "        -1.0830e+00,  1.1698e-01, -3.4169e-02, -7.0342e-01,  1.0932e+00,\n",
      "        -1.5132e+00,  4.7418e-01, -8.1649e-01,  6.9834e-01, -1.1079e+00,\n",
      "        -1.1196e+00,  1.2320e+00,  1.0540e-01, -4.2402e-01,  2.6490e-01,\n",
      "        -3.4179e-01,  1.2152e+00,  4.1531e-01, -1.7933e+00, -5.8630e-02,\n",
      "        -3.4828e-01,  1.9136e-01,  6.0939e-01, -5.8915e-01, -8.4627e-01,\n",
      "         3.8768e-01,  5.0225e-02, -1.5636e-01, -3.7990e-01, -1.4397e-01,\n",
      "        -4.3956e-01, -3.6644e-02,  3.9146e-01,  1.1234e+00, -1.5707e+00,\n",
      "        -1.3412e+00, -1.0952e+00,  1.8899e-01, -2.7015e+00,  6.2977e-01,\n",
      "        -1.4042e-01,  2.8649e-01,  6.3594e-01,  6.1251e-02,  2.9926e-01,\n",
      "         4.1174e-01, -2.5452e-01, -4.7790e-01, -1.4842e-01,  2.8987e-02,\n",
      "         1.7393e-01,  5.4744e-01,  4.4667e-02, -1.7993e-01,  7.4249e-01,\n",
      "        -2.9704e-01,  3.4378e-01, -2.6330e-01,  1.0131e+00, -8.9129e-01,\n",
      "        -2.3721e-02,  2.7531e-01,  1.8356e-01,  7.6636e-01, -1.7368e-01,\n",
      "         5.6646e-01,  1.2159e+00,  3.5576e-01,  2.0316e-01, -6.0107e-02,\n",
      "         7.1822e-01, -1.2945e-01, -3.2068e-01, -6.7184e-01,  7.5909e-01,\n",
      "         5.0962e-01,  8.8232e-01, -5.2475e-01, -5.9834e-01, -3.5976e-01,\n",
      "        -4.7668e-01, -3.8678e-01,  2.3469e-01,  3.0678e-01, -2.4896e-02,\n",
      "        -4.8672e-01,  1.2892e+00,  1.5046e+00, -6.7389e-01,  6.8612e-01,\n",
      "         2.4584e-02,  6.8452e-01,  5.7767e-01,  5.2325e-01,  1.9233e-01,\n",
      "         9.7426e-01,  4.7114e-02,  4.7954e-01,  4.0537e-02, -4.0769e-02,\n",
      "        -8.3592e-01,  7.0256e-01,  7.6637e-01,  5.6338e-01,  5.8097e-01,\n",
      "         8.0496e-01, -5.3367e-01,  4.8584e-01, -1.5728e-01, -9.4478e-01,\n",
      "         1.1926e+00,  5.5034e-01, -1.3355e+00, -5.9969e-01, -6.5493e-01,\n",
      "         2.4563e-01, -3.4545e-01,  1.6127e-01,  7.1841e-01, -9.3089e-01,\n",
      "         4.4266e-01,  4.5890e-01,  1.0778e+00,  2.2677e+00, -4.1448e-01,\n",
      "         4.8100e-01,  1.4988e+00,  9.3322e-01, -9.7011e-01,  2.9756e-01,\n",
      "         4.0917e-02,  3.2447e-01, -7.6698e-01,  4.3748e-01,  1.1181e+00,\n",
      "         1.0649e-01,  8.1863e-01, -8.4058e-01,  1.8906e-01,  8.4732e-01,\n",
      "         2.0598e+00, -3.5787e-01,  5.7401e-01, -3.6052e-01, -1.1555e+00,\n",
      "        -1.2010e-01,  5.5122e-01, -8.8001e-01, -4.8224e-01, -3.5766e-01,\n",
      "        -1.4321e+00,  3.6758e-01, -6.9261e-01,  2.8020e-01,  1.8237e-01,\n",
      "         7.8736e-01,  5.5919e-01, -1.9522e-01,  1.2480e+00,  1.9598e-01,\n",
      "         1.9535e+00, -6.4246e-01, -1.5658e-02, -2.7994e-01,  1.5930e-01,\n",
      "         1.7171e-01,  1.5765e+00,  9.6485e-01, -6.0651e-02,  3.7454e-01,\n",
      "         3.3513e-01,  7.8092e-01, -2.6718e-01, -5.5345e-01, -2.2483e-01,\n",
      "        -4.7379e-01, -3.8963e-03,  7.0742e-01,  6.7253e-01, -9.8426e-01,\n",
      "         5.1378e-01,  9.8561e-01,  5.6448e-01,  7.5835e-01, -2.9731e-01,\n",
      "        -8.3579e-01,  6.1841e-01, -2.7562e-02, -3.1357e-01,  8.2651e-02,\n",
      "        -3.6701e-03, -2.9639e-01, -2.1399e-02])\n"
     ]
    }
   ],
   "source": [
    "out = tokenizer(messages, \n",
    "                padding=True, \n",
    "                max_length=512, \n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\")\n",
    "\n",
    "bart_model = BartModel.from_pretrained(\"facebook/bart-base\")\n",
    "with torch.no_grad():\n",
    "    bart_model.eval()\n",
    "    print(out)\n",
    "    # print(*out)\n",
    "    # pred = bart_model(**out) # This sentence is same as below\n",
    "    pred = bart_model(\n",
    "        input_ids=out[\"input_ids\"], \n",
    "        attention_mask=out[\"attention_mask\"]\n",
    "    )\n",
    "    print(pred.last_hidden_state.shape) \n",
    "    embeddings = pred.last_hidden_state.mean(dim=1)\n",
    "    print(embeddings.shape)\n",
    "    print(embeddings[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a368a148",
   "metadata": {},
   "source": [
    "torch.Size([3, 16, 768])\n",
    "there are 3 dimensions here.\n",
    "our LLM predicted 1 token at a time.\n",
    "So it took 16 iterations for each sentence\n",
    "\n",
    "\n",
    "torch.Size([3, 16, 768]) Breakdown:\n",
    "\n",
    "3 — Batch size: You passed in a batch of 3 sequences (probably 3 sentences or documents).\n",
    "\n",
    "16 — Sequence length: Each input sequence is padded or truncated to a length of 16 tokens.\n",
    "\n",
    "768 — Hidden size: Each token in the sequence is represented by a 768-dimensional vector. This is a common hidden size for BART-base (similar to BERT-base).\n",
    "\n",
    "what is last_hidden_state?\n",
    "\n",
    "It's a tensor containing the final hidden layer's output for each token in each sequence.\n",
    "\n",
    "You can think of it as a 3D matrix:\\\n",
    "For each of the 3 input sequences →\\\n",
    "For each of the 16 tokens in the sequence →\\\n",
    "There's a 768-dimensional vector representing the model's internal understanding at that position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "74b0263c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "messages = [\n",
    "    \"We have release a new product, do you want to buy it?\", \n",
    "    \"Winner! Great deal, call us to get this product for free\",\n",
    "    \"Tomorrow is my birthday, do you come to the party?\",\n",
    "]\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "bart_model = BartModel.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def convert_to_embeddings(messages):\n",
    "    embeddings_list = []\n",
    "    for message in tqdm(messages):\n",
    "        out = tokenizer([message], \n",
    "                        padding=True,\n",
    "                        max_length=512, \n",
    "                        truncation=True, \n",
    "                        return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bart_model.eval()\n",
    "\n",
    "            pred = bart_model(\n",
    "                input_ids=out[\"input_ids\"], \n",
    "                attention_mask=out[\"attention_mask\"]\n",
    "            )\n",
    "            embeddings = pred.last_hidden_state.mean(dim=1)\\\n",
    "                .reshape((-1)) # It is going to matrix ([1, 768]) to a vector([768]) or tensor, so it can be easily processed by the neuron \n",
    "            embeddings_list.append(embeddings)\n",
    "    return torch.stack(embeddings_list)\n",
    "X = convert_to_embeddings(messages)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da54aaa3",
   "metadata": {},
   "source": [
    "Now implement this code in the neuron. Here instead of Count Vectorizer we are using Bart model. The reason it works more accurate because it is working with meanings(LLM embeddings) not just the words(CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "55e75903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4458/4458 [11:53<00:00,  6.25it/s]\n",
      "100%|██████████| 1114/1114 [02:13<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1423, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0268, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0200, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0164, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0140, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0124, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0111, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0100, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0092, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0085, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Evaluating on the training data\n",
      "accuracy: tensor(0.9989)\n",
      "sensitivity: tensor(0.9984)\n",
      "specificity: tensor(0.9990)\n",
      "precision: tensor(0.9935)\n",
      "Evaluating on the validation data\n",
      "accuracy: tensor(0.9928)\n",
      "sensitivity: tensor(0.9856)\n",
      "specificity: tensor(0.9938)\n",
      "precision: tensor(0.9580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.2270e-03],\n",
      "        [8.5227e-01],\n",
      "        [5.1804e-07]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "bart_model = BartModel.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def convert_to_embeddings(messages):\n",
    "    embeddings_list = []\n",
    "    for message in tqdm(messages):\n",
    "        out = tokenizer([message], \n",
    "                        padding=True,\n",
    "                        max_length=512, \n",
    "                        truncation=True, \n",
    "                        return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bart_model.eval()\n",
    "\n",
    "            pred = bart_model(\n",
    "                input_ids=out[\"input_ids\"], \n",
    "                attention_mask=out[\"attention_mask\"]\n",
    "            )\n",
    "            embeddings = pred.last_hidden_state.mean(dim=1)\\\n",
    "                .reshape((-1))\n",
    "            embeddings_list.append(embeddings)\n",
    "    return torch.stack(embeddings_list)\n",
    "\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = df[\"type\"] == \"spam\"\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "df_train = df.sample(frac=0.8, random_state=0)\n",
    "df_val = df.drop(index=df_train.index)\n",
    "\n",
    "X_train = convert_to_embeddings(df_train[\"message\"].tolist())\n",
    "X_val = convert_to_embeddings(df_val[\"message\"].tolist())\n",
    "\n",
    "y_train = torch.tensor(df_train[\"spam\"].values, dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "y_val = torch.tensor(df_val[\"spam\"].values, dtype=torch.float32)\\\n",
    "        .reshape((-1, 1))\n",
    "\n",
    "model = nn.Linear(768, 1)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        print(loss)\n",
    "\n",
    "def evaluate_model(X, y): \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = nn.functional.sigmoid(model(X)) > 0.25\n",
    "        print(\"accuracy:\", (y_pred == y)\\\n",
    "            .type(torch.float32).mean())\n",
    "        \n",
    "        print(\"sensitivity:\", (y_pred[y == 1] == y[y == 1])\\\n",
    "            .type(torch.float32).mean())\n",
    "        \n",
    "        print(\"specificity:\", (y_pred[y == 0] == y[y == 0])\\\n",
    "            .type(torch.float32).mean())\n",
    "\n",
    "        print(\"precision:\", (y_pred[y_pred == 1] == y[y_pred == 1])\\\n",
    "            .type(torch.float32).mean()) \n",
    "        \n",
    "print(\"Evaluating on the training data\")\n",
    "evaluate_model(X_train, y_train)\n",
    "\n",
    "print(\"Evaluating on the validation data\")\n",
    "evaluate_model(X_val, y_val)\n",
    "\n",
    "X_custom = convert_to_embeddings([\n",
    "    \"We have release a new product, do you want to buy it?\", \n",
    "    \"Winner! Great deal, call us to get this product for free\",\n",
    "    \"Tomorrow is my birthday, do you come to the party?\"\n",
    "])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = nn.functional.sigmoid(model(X_custom))\n",
    "    print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed0dc3",
   "metadata": {},
   "source": [
    "### Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "476232ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and transforming training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4457 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "  4%|▍         | 173/4457 [03:06<1:16:49,  1.08s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens_train[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m]))):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         output = \u001b[43mmodel_bart\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokens_train\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokens_train\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m         embeddings_train.append(output.last_hidden_state.mean(dim=\u001b[32m1\u001b[39m).squeeze(\u001b[32m0\u001b[39m))\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Transform validation data with progress bar\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1288\u001b[39m, in \u001b[36mBartModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1281\u001b[39m     encoder_outputs = BaseModelOutput(\n\u001b[32m   1282\u001b[39m         last_hidden_state=encoder_outputs[\u001b[32m0\u001b[39m],\n\u001b[32m   1283\u001b[39m         hidden_states=encoder_outputs[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1284\u001b[39m         attentions=encoder_outputs[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1285\u001b[39m     )\n\u001b[32m   1287\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1288\u001b[39m decoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1289\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1297\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1304\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m   1305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs + encoder_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1115\u001b[39m, in \u001b[36mBartDecoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dropout_probability < \u001b[38;5;28mself\u001b[39m.layerdrop:\n\u001b[32m   1113\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1115\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m   1119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1127\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         logger.warning(message)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\models\\bart\\modeling_bart.py:430\u001b[39m, in \u001b[36mBartDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache, cache_position)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    428\u001b[39m     residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     hidden_states, cross_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m     hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n\u001b[32m    440\u001b[39m     hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\models\\bart\\modeling_bart.py:235\u001b[39m, in \u001b[36mBartAttention.forward\u001b[39m\u001b[34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions, cache_position, **kwargs)\u001b[39m\n\u001b[32m    233\u001b[39m     value_states = curr_past_key_value.layers[\u001b[38;5;28mself\u001b[39m.layer_idx].values\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     key_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m     value_states = \u001b[38;5;28mself\u001b[39m.v_proj(current_states)\n\u001b[32m    237\u001b[39m     key_states = key_states.view(*kv_input_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"./data/SMSSpamCollection\", \n",
    "                 sep=\"\\t\", \n",
    "                 names=[\"type\", \"message\"])\n",
    "\n",
    "df[\"spam\"] = (df[\"type\"] == \"spam\").astype(int)\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "# Split the data\n",
    "df_train, df_val = train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "# Load BART and tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model_bart = BartModel.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# Tokenize messages with a progress bar\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(texts.tolist(), \n",
    "                     padding=True, \n",
    "                     truncation=True, \n",
    "                     return_tensors=\"pt\", \n",
    "                     max_length=512)\n",
    "\n",
    "# Transform training data with progress bar\n",
    "print(\"Tokenizing and transforming training data...\")\n",
    "tokens_train = tokenize_texts(df_train[\"message\"])\n",
    "embeddings_train = []\n",
    "for i in tqdm(range(len(tokens_train['input_ids']))):\n",
    "    with torch.no_grad():\n",
    "        output = model_bart(input_ids=tokens_train['input_ids'][i].unsqueeze(0), \n",
    "                            attention_mask=tokens_train['attention_mask'][i].unsqueeze(0))\n",
    "        embeddings_train.append(output.last_hidden_state.mean(dim=1).squeeze(0))\n",
    "\n",
    "# Transform validation data with progress bar\n",
    "print(\"Tokenizing and transforming validation data...\")\n",
    "tokens_val = tokenize_texts(df_val[\"message\"])\n",
    "embeddings_val = []\n",
    "for i in tqdm(range(len(tokens_val['input_ids']))):\n",
    "    with torch.no_grad():\n",
    "        output = model_bart(input_ids=tokens_val['input_ids'][i].unsqueeze(0), \n",
    "                            attention_mask=tokens_val['attention_mask'][i].unsqueeze(0))\n",
    "        embeddings_val.append(output.last_hidden_state.mean(dim=1).squeeze(0))\n",
    "\n",
    "# Stack the embeddings into tensors\n",
    "X_train = torch.stack(embeddings_train)\n",
    "y_train = torch.tensor(df_train[\"spam\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_val = torch.stack(embeddings_val)\n",
    "y_val = torch.tensor(df_val[\"spam\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Define a simple linear model\n",
    "model = nn.Linear(X_train.size(1), 1)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "\n",
    "# Training loop\n",
    "for i in range(10000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        print(f\"Iteration {i}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(X, y): \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = torch.sigmoid(model(X)) > 0.25\n",
    "        accuracy = (y_pred == y).type(torch.float32).mean().item()\n",
    "        sensitivity = (y_pred[y == 1] == y[y == 1]).type(torch.float32).mean().item()\n",
    "        specificity = (y_pred[y == 0] == y[y == 0]).type(torch.float32).mean().item()\n",
    "        precision = (y_pred[y_pred == 1] == y[y_pred == 1]).type(torch.float32).mean().item()\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Sensitivity: {sensitivity}\")\n",
    "        print(f\"Specificity: {specificity}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "\n",
    "# Evaluate on training data\n",
    "print(\"Evaluating on the training data\")\n",
    "evaluate_model(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation data\n",
    "print(\"Evaluating on the validation data\")\n",
    "evaluate_model(X_val, y_val)\n",
    "\n",
    "# Custom messages for prediction\n",
    "custom_messages = pd.Series([\n",
    "    \"We have released a new product, do you want to buy it?\", \n",
    "    \"Winner! Great deal, call us to get this product for free\",\n",
    "    \"Tomorrow is my birthday, do you want to come to the party?\"\n",
    "])\n",
    "\n",
    "tokens_custom = tokenize_texts(custom_messages)\n",
    "embeddings_custom = []\n",
    "print(\"Transforming custom messages...\")\n",
    "for i in tqdm(range(len(tokens_custom['input_ids']))):\n",
    "    with torch.no_grad():\n",
    "        output = model_bart(input_ids=tokens_custom['input_ids'][i].unsqueeze(0), \n",
    "                            attention_mask=tokens_custom['attention_mask'][i].unsqueeze(0))\n",
    "        embeddings_custom.append(output.last_hidden_state.mean(dim=1).squeeze(0))\n",
    "\n",
    "X_custom = torch.stack(embeddings_custom)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    pred = torch.sigmoid(model(X_custom))\n",
    "    print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5d2ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
